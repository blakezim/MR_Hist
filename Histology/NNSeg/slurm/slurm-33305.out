python is /home/sci/blakez/software/anaconda3/envs/MR_Hist/bin/python
Running with GPU Index 0,1
Namespace(cuda=False, data_dir='/usr/sci/scratch/blakez/microscopic_data/', im_size=256, inferBatchSize=64, lr=0.0001, nEpochs=1000, out_dir='/usr/sci/scratch/blakez/microscopic_data/Output/', repeat_factor=40, seed=358, threads=4, trainBatchSize=100)
===> Generating Datasets ...  done
===> Beginning Training
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 1.759429
=> Done with 2 / 34  Batch Loss: 1.689243
=> Done with 3 / 34  Batch Loss: 1.679370
=> Done with 4 / 34  Batch Loss: 1.683280
=> Done with 5 / 34  Batch Loss: 1.654710
=> Done with 6 / 34  Batch Loss: 1.631871
=> Done with 7 / 34  Batch Loss: 1.623986
=> Done with 8 / 34  Batch Loss: 1.625978
=> Done with 9 / 34  Batch Loss: 1.581499
=> Done with 10 / 34  Batch Loss: 1.558693
=> Done with 11 / 34  Batch Loss: 1.574020
=> Done with 12 / 34  Batch Loss: 1.537154
=> Done with 13 / 34  Batch Loss: 1.529310
=> Done with 14 / 34  Batch Loss: 1.561839
=> Done with 15 / 34  Batch Loss: 1.495351
=> Done with 16 / 34  Batch Loss: 1.506478
=> Done with 17 / 34  Batch Loss: 1.497375
=> Done with 18 / 34  Batch Loss: 1.475941
=> Done with 19 / 34  Batch Loss: 1.519343
=> Done with 20 / 34  Batch Loss: 1.453800
=> Done with 21 / 34  Batch Loss: 1.469349
=> Done with 22 / 34  Batch Loss: 1.425427
=> Done with 23 / 34  Batch Loss: 1.431958
=> Done with 24 / 34  Batch Loss: 1.404676
=> Done with 25 / 34  Batch Loss: 1.414599
=> Done with 26 / 34  Batch Loss: 1.425306
=> Done with 27 / 34  Batch Loss: 1.465734
=> Done with 28 / 34  Batch Loss: 1.393022
=> Done with 29 / 34  Batch Loss: 1.358782
=> Done with 30 / 34  Batch Loss: 1.377711
=> Done with 31 / 34  Batch Loss: 1.343037
=> Done with 32 / 34  Batch Loss: 1.323524
=> Done with 33 / 34  Batch Loss: 1.328583
=> Done with 34 / 34  Batch Loss: 1.366709
===> Epoch 1 Complete: Avg. Loss: 1.504914
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.216375
=> Done with 2 / 34  Batch Loss: 1.231705
=> Done with 3 / 34  Batch Loss: 1.245184
=> Done with 4 / 34  Batch Loss: 1.220084
=> Done with 5 / 34  Batch Loss: 1.245918
=> Done with 6 / 34  Batch Loss: 1.230098
=> Done with 7 / 34  Batch Loss: 1.233880
=> Done with 8 / 34  Batch Loss: 1.220214
=> Done with 9 / 34  Batch Loss: 1.211046
=> Done with 10 / 34  Batch Loss: 1.213490
=> Done with 11 / 34  Batch Loss: 1.216350
=> Done with 12 / 34  Batch Loss: 1.220925
=> Done with 13 / 34  Batch Loss: 1.207235
=> Done with 14 / 34  Batch Loss: 1.230791
=> Done with 15 / 34  Batch Loss: 1.215865
=> Done with 16 / 34  Batch Loss: 1.205305
=> Done with 17 / 34  Batch Loss: 1.233203
=> Done with 18 / 34  Batch Loss: 1.217750
=> Done with 19 / 34  Batch Loss: 1.222356
=> Done with 20 / 34  Batch Loss: 1.233809
=> Done with 21 / 34  Batch Loss: 1.217407
=> Done with 22 / 34  Batch Loss: 1.240105
=> Done with 23 / 34  Batch Loss: 1.216696
=> Done with 24 / 34  Batch Loss: 1.229148
=> Done with 25 / 34  Batch Loss: 1.230168
=> Done with 26 / 34  Batch Loss: 1.225904
=> Done with 27 / 34  Batch Loss: 1.222870
=> Done with 28 / 34  Batch Loss: 1.240385
=> Done with 29 / 34  Batch Loss: 1.224022
=> Done with 30 / 34  Batch Loss: 1.228523
=> Done with 31 / 34  Batch Loss: 1.218211
=> Done with 32 / 34  Batch Loss: 1.222876
=> Done with 33 / 34  Batch Loss: 1.227172
=> Done with 34 / 34  Batch Loss: 1.235290
===> Avg. MSE Loss: 1.225011
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 1.351853
=> Done with 2 / 34  Batch Loss: 1.308277
=> Done with 3 / 34  Batch Loss: 1.333454
=> Done with 4 / 34  Batch Loss: 1.316501
=> Done with 5 / 34  Batch Loss: 1.288457
=> Done with 6 / 34  Batch Loss: 1.308665
=> Done with 7 / 34  Batch Loss: 1.343131
=> Done with 8 / 34  Batch Loss: 1.262622
=> Done with 9 / 34  Batch Loss: 1.277165
=> Done with 10 / 34  Batch Loss: 1.307038
=> Done with 11 / 34  Batch Loss: 1.260236
=> Done with 12 / 34  Batch Loss: 1.279863
=> Done with 13 / 34  Batch Loss: 1.231498
=> Done with 14 / 34  Batch Loss: 1.269047
=> Done with 15 / 34  Batch Loss: 1.241769
=> Done with 16 / 34  Batch Loss: 1.253461
=> Done with 17 / 34  Batch Loss: 1.250564
=> Done with 18 / 34  Batch Loss: 1.279271
=> Done with 19 / 34  Batch Loss: 1.216421
=> Done with 20 / 34  Batch Loss: 1.294111
=> Done with 21 / 34  Batch Loss: 1.201839
=> Done with 22 / 34  Batch Loss: 1.247492
=> Done with 23 / 34  Batch Loss: 1.235591
=> Done with 24 / 34  Batch Loss: 1.233397
=> Done with 25 / 34  Batch Loss: 1.209645
=> Done with 26 / 34  Batch Loss: 1.213269
=> Done with 27 / 34  Batch Loss: 1.151734
=> Done with 28 / 34  Batch Loss: 1.261687
=> Done with 29 / 34  Batch Loss: 1.229413
=> Done with 30 / 34  Batch Loss: 1.220386
=> Done with 31 / 34  Batch Loss: 1.202000
=> Done with 32 / 34  Batch Loss: 1.158008
=> Done with 33 / 34  Batch Loss: 1.226101
=> Done with 34 / 34  Batch Loss: 1.214042
===> Epoch 2 Complete: Avg. Loss: 1.255236
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.138528
=> Done with 2 / 34  Batch Loss: 1.106319
=> Done with 3 / 34  Batch Loss: 1.053713
=> Done with 4 / 34  Batch Loss: 1.068597
=> Done with 5 / 34  Batch Loss: 1.089596
=> Done with 6 / 34  Batch Loss: 1.111064
=> Done with 7 / 34  Batch Loss: 1.064074
=> Done with 8 / 34  Batch Loss: 1.131131
=> Done with 9 / 34  Batch Loss: 1.111688
=> Done with 10 / 34  Batch Loss: 1.072816
=> Done with 11 / 34  Batch Loss: 1.117371
=> Done with 12 / 34  Batch Loss: 1.143021
=> Done with 13 / 34  Batch Loss: 1.092317
=> Done with 14 / 34  Batch Loss: 1.097682
=> Done with 15 / 34  Batch Loss: 1.137247
=> Done with 16 / 34  Batch Loss: 1.052311
=> Done with 17 / 34  Batch Loss: 1.107220
=> Done with 18 / 34  Batch Loss: 1.096751
=> Done with 19 / 34  Batch Loss: 1.060679
=> Done with 20 / 34  Batch Loss: 1.100429
=> Done with 21 / 34  Batch Loss: 1.137290
=> Done with 22 / 34  Batch Loss: 1.082420
=> Done with 23 / 34  Batch Loss: 1.105154
=> Done with 24 / 34  Batch Loss: 1.061766
=> Done with 25 / 34  Batch Loss: 1.065760
=> Done with 26 / 34  Batch Loss: 1.077403
=> Done with 27 / 34  Batch Loss: 1.100273
=> Done with 28 / 34  Batch Loss: 1.127369
=> Done with 29 / 34  Batch Loss: 1.099980
=> Done with 30 / 34  Batch Loss: 1.078303
=> Done with 31 / 34  Batch Loss: 1.111268
=> Done with 32 / 34  Batch Loss: 1.072109
=> Done with 33 / 34  Batch Loss: 1.066329
=> Done with 34 / 34  Batch Loss: 1.085986
===> Avg. MSE Loss: 1.094823
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 1.243915
=> Done with 2 / 34  Batch Loss: 1.227669
=> Done with 3 / 34  Batch Loss: 1.186235
=> Done with 4 / 34  Batch Loss: 1.184201
=> Done with 5 / 34  Batch Loss: 1.203146
=> Done with 6 / 34  Batch Loss: 1.158477
=> Done with 7 / 34  Batch Loss: 1.205607
=> Done with 8 / 34  Batch Loss: 1.231748
=> Done with 9 / 34  Batch Loss: 1.176077
=> Done with 10 / 34  Batch Loss: 1.206557
=> Done with 11 / 34  Batch Loss: 1.158314
=> Done with 12 / 34  Batch Loss: 1.169611
=> Done with 13 / 34  Batch Loss: 1.196682
=> Done with 14 / 34  Batch Loss: 1.162063
=> Done with 15 / 34  Batch Loss: 1.160315
=> Done with 16 / 34  Batch Loss: 1.130178
=> Done with 17 / 34  Batch Loss: 1.173706
=> Done with 18 / 34  Batch Loss: 1.164095
=> Done with 19 / 34  Batch Loss: 1.194380
=> Done with 20 / 34  Batch Loss: 1.138533
=> Done with 21 / 34  Batch Loss: 1.156906
=> Done with 22 / 34  Batch Loss: 1.153248
=> Done with 23 / 34  Batch Loss: 1.156778
=> Done with 24 / 34  Batch Loss: 1.141080
=> Done with 25 / 34  Batch Loss: 1.128694
=> Done with 26 / 34  Batch Loss: 1.131135
=> Done with 27 / 34  Batch Loss: 1.143479
=> Done with 28 / 34  Batch Loss: 1.130720
=> Done with 29 / 34  Batch Loss: 1.109132
=> Done with 30 / 34  Batch Loss: 1.118526
=> Done with 31 / 34  Batch Loss: 1.130191
=> Done with 32 / 34  Batch Loss: 1.101313
=> Done with 33 / 34  Batch Loss: 1.108733
=> Done with 34 / 34  Batch Loss: 1.139970
===> Epoch 3 Complete: Avg. Loss: 1.162394
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.210204
=> Done with 2 / 34  Batch Loss: 1.174634
=> Done with 3 / 34  Batch Loss: 1.155784
=> Done with 4 / 34  Batch Loss: 1.208251
=> Done with 5 / 34  Batch Loss: 1.177320
=> Done with 6 / 34  Batch Loss: 1.233657
=> Done with 7 / 34  Batch Loss: 1.166984
=> Done with 8 / 34  Batch Loss: 1.153667
=> Done with 9 / 34  Batch Loss: 1.189411
=> Done with 10 / 34  Batch Loss: 1.178640
=> Done with 11 / 34  Batch Loss: 1.136513
=> Done with 12 / 34  Batch Loss: 1.199597
=> Done with 13 / 34  Batch Loss: 1.175139
=> Done with 14 / 34  Batch Loss: 1.174057
=> Done with 15 / 34  Batch Loss: 1.146677
=> Done with 16 / 34  Batch Loss: 1.185403
=> Done with 17 / 34  Batch Loss: 1.177428
=> Done with 18 / 34  Batch Loss: 1.200961
=> Done with 19 / 34  Batch Loss: 1.188526
=> Done with 20 / 34  Batch Loss: 1.165067
=> Done with 21 / 34  Batch Loss: 1.182130
=> Done with 22 / 34  Batch Loss: 1.206810
=> Done with 23 / 34  Batch Loss: 1.183345
=> Done with 24 / 34  Batch Loss: 1.173544
=> Done with 25 / 34  Batch Loss: 1.191102
=> Done with 26 / 34  Batch Loss: 1.152357
=> Done with 27 / 34  Batch Loss: 1.137351
=> Done with 28 / 34  Batch Loss: 1.193321
=> Done with 29 / 34  Batch Loss: 1.174576
=> Done with 30 / 34  Batch Loss: 1.148995
=> Done with 31 / 34  Batch Loss: 1.165551
=> Done with 32 / 34  Batch Loss: 1.201910
=> Done with 33 / 34  Batch Loss: 1.176528
=> Done with 34 / 34  Batch Loss: 1.175573
===> Avg. MSE Loss: 1.178265
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 1.109313
=> Done with 2 / 34  Batch Loss: 1.106049
=> Done with 3 / 34  Batch Loss: 1.092470
=> Done with 4 / 34  Batch Loss: 1.105604
=> Done with 5 / 34  Batch Loss: 1.108744
=> Done with 6 / 34  Batch Loss: 1.118150
=> Done with 7 / 34  Batch Loss: 1.112432
=> Done with 8 / 34  Batch Loss: 1.089682
=> Done with 9 / 34  Batch Loss: 1.097759
=> Done with 10 / 34  Batch Loss: 1.099852
=> Done with 11 / 34  Batch Loss: 1.108394
=> Done with 12 / 34  Batch Loss: 1.101620
=> Done with 13 / 34  Batch Loss: 1.089494
=> Done with 14 / 34  Batch Loss: 1.094332
=> Done with 15 / 34  Batch Loss: 1.097366
=> Done with 16 / 34  Batch Loss: 1.108388
=> Done with 17 / 34  Batch Loss: 1.088699
=> Done with 18 / 34  Batch Loss: 1.104925
=> Done with 19 / 34  Batch Loss: 1.068678
=> Done with 20 / 34  Batch Loss: 1.098549
=> Done with 21 / 34  Batch Loss: 1.067987
=> Done with 22 / 34  Batch Loss: 1.093153
=> Done with 23 / 34  Batch Loss: 1.061018
=> Done with 24 / 34  Batch Loss: 1.069616
=> Done with 25 / 34  Batch Loss: 1.061917
=> Done with 26 / 34  Batch Loss: 1.066549
=> Done with 27 / 34  Batch Loss: 1.066162
=> Done with 28 / 34  Batch Loss: 1.063841
=> Done with 29 / 34  Batch Loss: 1.065572
=> Done with 30 / 34  Batch Loss: 1.062637
=> Done with 31 / 34  Batch Loss: 1.077211
=> Done with 32 / 34  Batch Loss: 1.075794
=> Done with 33 / 34  Batch Loss: 1.083100
=> Done with 34 / 34  Batch Loss: 1.103113
===> Epoch 4 Complete: Avg. Loss: 1.088770
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.216053
=> Done with 2 / 34  Batch Loss: 1.205511
=> Done with 3 / 34  Batch Loss: 1.170264
=> Done with 4 / 34  Batch Loss: 1.205626
=> Done with 5 / 34  Batch Loss: 1.218280
=> Done with 6 / 34  Batch Loss: 1.229205
=> Done with 7 / 34  Batch Loss: 1.213364
=> Done with 8 / 34  Batch Loss: 1.221162
=> Done with 9 / 34  Batch Loss: 1.231085
=> Done with 10 / 34  Batch Loss: 1.232696
=> Done with 11 / 34  Batch Loss: 1.238265
=> Done with 12 / 34  Batch Loss: 1.227694
=> Done with 13 / 34  Batch Loss: 1.206669
=> Done with 14 / 34  Batch Loss: 1.234919
=> Done with 15 / 34  Batch Loss: 1.201044
=> Done with 16 / 34  Batch Loss: 1.190356
=> Done with 17 / 34  Batch Loss: 1.241476
=> Done with 18 / 34  Batch Loss: 1.187983
=> Done with 19 / 34  Batch Loss: 1.245445
=> Done with 20 / 34  Batch Loss: 1.243092
=> Done with 21 / 34  Batch Loss: 1.210580
=> Done with 22 / 34  Batch Loss: 1.211470
=> Done with 23 / 34  Batch Loss: 1.226802
=> Done with 24 / 34  Batch Loss: 1.268269
=> Done with 25 / 34  Batch Loss: 1.219038
=> Done with 26 / 34  Batch Loss: 1.262858
=> Done with 27 / 34  Batch Loss: 1.234499
=> Done with 28 / 34  Batch Loss: 1.222335
=> Done with 29 / 34  Batch Loss: 1.254176
=> Done with 30 / 34  Batch Loss: 1.196398
=> Done with 31 / 34  Batch Loss: 1.234156
=> Done with 32 / 34  Batch Loss: 1.274621
=> Done with 33 / 34  Batch Loss: 1.260372
=> Done with 34 / 34  Batch Loss: 1.259646
===> Avg. MSE Loss: 1.226336
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 1.045896
=> Done with 2 / 34  Batch Loss: 1.053923
=> Done with 3 / 34  Batch Loss: 1.066537
=> Done with 4 / 34  Batch Loss: 1.097785
=> Done with 5 / 34  Batch Loss: 1.060973
=> Done with 6 / 34  Batch Loss: 1.092391
=> Done with 7 / 34  Batch Loss: 1.082970
=> Done with 8 / 34  Batch Loss: 1.047469
=> Done with 9 / 34  Batch Loss: 1.081963
=> Done with 10 / 34  Batch Loss: 1.097345
=> Done with 11 / 34  Batch Loss: 1.078448
=> Done with 12 / 34  Batch Loss: 1.048911
=> Done with 13 / 34  Batch Loss: 1.049891
=> Done with 14 / 34  Batch Loss: 1.068340
=> Done with 15 / 34  Batch Loss: 1.093042
=> Done with 16 / 34  Batch Loss: 1.065327
=> Done with 17 / 34  Batch Loss: 1.047503
=> Done with 18 / 34  Batch Loss: 1.051343
=> Done with 19 / 34  Batch Loss: 1.032869
=> Done with 20 / 34  Batch Loss: 1.046271
=> Done with 21 / 34  Batch Loss: 1.035283
=> Done with 22 / 34  Batch Loss: 1.035888
=> Done with 23 / 34  Batch Loss: 1.034917
=> Done with 24 / 34  Batch Loss: 1.027857
=> Done with 25 / 34  Batch Loss: 1.048113
=> Done with 26 / 34  Batch Loss: 1.043664
=> Done with 27 / 34  Batch Loss: 1.057015
=> Done with 28 / 34  Batch Loss: 1.012400
=> Done with 29 / 34  Batch Loss: 1.007295
=> Done with 30 / 34  Batch Loss: 1.034210
=> Done with 31 / 34  Batch Loss: 1.019852
=> Done with 32 / 34  Batch Loss: 1.038082
=> Done with 33 / 34  Batch Loss: 1.011413
=> Done with 34 / 34  Batch Loss: 1.019882
===> Epoch 5 Complete: Avg. Loss: 1.051032
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.253276
=> Done with 2 / 34  Batch Loss: 1.252871
=> Done with 3 / 34  Batch Loss: 1.266415
=> Done with 4 / 34  Batch Loss: 1.239320
=> Done with 5 / 34  Batch Loss: 1.227470
=> Done with 6 / 34  Batch Loss: 1.223241
=> Done with 7 / 34  Batch Loss: 1.248915
=> Done with 8 / 34  Batch Loss: 1.244071
=> Done with 9 / 34  Batch Loss: 1.256603
=> Done with 10 / 34  Batch Loss: 1.208986
=> Done with 11 / 34  Batch Loss: 1.191681
=> Done with 12 / 34  Batch Loss: 1.241342
=> Done with 13 / 34  Batch Loss: 1.270202
=> Done with 14 / 34  Batch Loss: 1.215460
=> Done with 15 / 34  Batch Loss: 1.245232
=> Done with 16 / 34  Batch Loss: 1.247360
=> Done with 17 / 34  Batch Loss: 1.190209
=> Done with 18 / 34  Batch Loss: 1.178573
=> Done with 19 / 34  Batch Loss: 1.254819
=> Done with 20 / 34  Batch Loss: 1.247181
=> Done with 21 / 34  Batch Loss: 1.229783
=> Done with 22 / 34  Batch Loss: 1.233422
=> Done with 23 / 34  Batch Loss: 1.240507
=> Done with 24 / 34  Batch Loss: 1.232732
=> Done with 25 / 34  Batch Loss: 1.226872
=> Done with 26 / 34  Batch Loss: 1.223365
=> Done with 27 / 34  Batch Loss: 1.243820
=> Done with 28 / 34  Batch Loss: 1.247123
=> Done with 29 / 34  Batch Loss: 1.184142
=> Done with 30 / 34  Batch Loss: 1.221232
=> Done with 31 / 34  Batch Loss: 1.206361
=> Done with 32 / 34  Batch Loss: 1.243149
=> Done with 33 / 34  Batch Loss: 1.244649
=> Done with 34 / 34  Batch Loss: 1.230119
===> Avg. MSE Loss: 1.232662
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 1.008106
=> Done with 2 / 34  Batch Loss: 1.021504
=> Done with 3 / 34  Batch Loss: 1.042814
=> Done with 4 / 34  Batch Loss: 1.014372
=> Done with 5 / 34  Batch Loss: 1.035084
=> Done with 6 / 34  Batch Loss: 1.005533
=> Done with 7 / 34  Batch Loss: 1.022395
=> Done with 8 / 34  Batch Loss: 0.996280
=> Done with 9 / 34  Batch Loss: 1.020639
=> Done with 10 / 34  Batch Loss: 0.991247
=> Done with 11 / 34  Batch Loss: 0.994919
=> Done with 12 / 34  Batch Loss: 1.006583
=> Done with 13 / 34  Batch Loss: 0.991053
=> Done with 14 / 34  Batch Loss: 1.008939
=> Done with 15 / 34  Batch Loss: 1.001516
=> Done with 16 / 34  Batch Loss: 1.001135
=> Done with 17 / 34  Batch Loss: 1.008169
=> Done with 18 / 34  Batch Loss: 1.006100
=> Done with 19 / 34  Batch Loss: 0.989239
=> Done with 20 / 34  Batch Loss: 0.994739
=> Done with 21 / 34  Batch Loss: 0.997873
=> Done with 22 / 34  Batch Loss: 0.980112
=> Done with 23 / 34  Batch Loss: 0.983018
=> Done with 24 / 34  Batch Loss: 0.962506
=> Done with 25 / 34  Batch Loss: 0.989522
=> Done with 26 / 34  Batch Loss: 0.985662
=> Done with 27 / 34  Batch Loss: 0.955523
=> Done with 28 / 34  Batch Loss: 0.970739
=> Done with 29 / 34  Batch Loss: 0.972875
=> Done with 30 / 34  Batch Loss: 0.965705
=> Done with 31 / 34  Batch Loss: 0.977641
=> Done with 32 / 34  Batch Loss: 0.975995
=> Done with 33 / 34  Batch Loss: 0.984110
=> Done with 34 / 34  Batch Loss: 0.965220
===> Epoch 6 Complete: Avg. Loss: 0.994908
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.194719
=> Done with 2 / 34  Batch Loss: 1.172792
=> Done with 3 / 34  Batch Loss: 1.296656
=> Done with 4 / 34  Batch Loss: 1.252758
=> Done with 5 / 34  Batch Loss: 1.253284
=> Done with 6 / 34  Batch Loss: 1.263879
=> Done with 7 / 34  Batch Loss: 1.280265
=> Done with 8 / 34  Batch Loss: 1.238038
=> Done with 9 / 34  Batch Loss: 1.240799
=> Done with 10 / 34  Batch Loss: 1.238956
=> Done with 11 / 34  Batch Loss: 1.258512
=> Done with 12 / 34  Batch Loss: 1.262085
=> Done with 13 / 34  Batch Loss: 1.242114
=> Done with 14 / 34  Batch Loss: 1.214652
=> Done with 15 / 34  Batch Loss: 1.228876
=> Done with 16 / 34  Batch Loss: 1.246518
=> Done with 17 / 34  Batch Loss: 1.264366
=> Done with 18 / 34  Batch Loss: 1.231337
=> Done with 19 / 34  Batch Loss: 1.253250
=> Done with 20 / 34  Batch Loss: 1.264480
=> Done with 21 / 34  Batch Loss: 1.269013
=> Done with 22 / 34  Batch Loss: 1.255659
=> Done with 23 / 34  Batch Loss: 1.219984
=> Done with 24 / 34  Batch Loss: 1.190780
=> Done with 25 / 34  Batch Loss: 1.275823
=> Done with 26 / 34  Batch Loss: 1.197555
=> Done with 27 / 34  Batch Loss: 1.227169
=> Done with 28 / 34  Batch Loss: 1.278522
=> Done with 29 / 34  Batch Loss: 1.274502
=> Done with 30 / 34  Batch Loss: 1.259914
=> Done with 31 / 34  Batch Loss: 1.276480
=> Done with 32 / 34  Batch Loss: 1.213152
=> Done with 33 / 34  Batch Loss: 1.288283
=> Done with 34 / 34  Batch Loss: 1.244123
===> Avg. MSE Loss: 1.246156
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.978023
=> Done with 2 / 34  Batch Loss: 0.973997
=> Done with 3 / 34  Batch Loss: 0.973645
=> Done with 4 / 34  Batch Loss: 0.960407
=> Done with 5 / 34  Batch Loss: 0.968492
=> Done with 6 / 34  Batch Loss: 0.936828
=> Done with 7 / 34  Batch Loss: 0.934956
=> Done with 8 / 34  Batch Loss: 0.948411
=> Done with 9 / 34  Batch Loss: 0.962200
=> Done with 10 / 34  Batch Loss: 0.957702
=> Done with 11 / 34  Batch Loss: 0.954142
=> Done with 12 / 34  Batch Loss: 0.959463
=> Done with 13 / 34  Batch Loss: 0.943807
=> Done with 14 / 34  Batch Loss: 0.969755
=> Done with 15 / 34  Batch Loss: 0.947284
=> Done with 16 / 34  Batch Loss: 0.984644
=> Done with 17 / 34  Batch Loss: 0.955208
=> Done with 18 / 34  Batch Loss: 0.937689
=> Done with 19 / 34  Batch Loss: 0.957110
=> Done with 20 / 34  Batch Loss: 0.965367
=> Done with 21 / 34  Batch Loss: 0.954511
=> Done with 22 / 34  Batch Loss: 0.943108
=> Done with 23 / 34  Batch Loss: 0.937725
=> Done with 24 / 34  Batch Loss: 0.942101
=> Done with 25 / 34  Batch Loss: 0.930355
=> Done with 26 / 34  Batch Loss: 0.934303
=> Done with 27 / 34  Batch Loss: 0.913787
=> Done with 28 / 34  Batch Loss: 0.960037
=> Done with 29 / 34  Batch Loss: 0.925476
=> Done with 30 / 34  Batch Loss: 0.963741
=> Done with 31 / 34  Batch Loss: 0.945327
=> Done with 32 / 34  Batch Loss: 0.944434
=> Done with 33 / 34  Batch Loss: 0.923778
=> Done with 34 / 34  Batch Loss: 0.925515
===> Epoch 7 Complete: Avg. Loss: 0.950392
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.258561
=> Done with 2 / 34  Batch Loss: 1.219040
=> Done with 3 / 34  Batch Loss: 1.130044
=> Done with 4 / 34  Batch Loss: 1.161956
=> Done with 5 / 34  Batch Loss: 1.155299
=> Done with 6 / 34  Batch Loss: 1.151650
=> Done with 7 / 34  Batch Loss: 1.199346
=> Done with 8 / 34  Batch Loss: 1.165905
=> Done with 9 / 34  Batch Loss: 1.254236
=> Done with 10 / 34  Batch Loss: 1.181949
=> Done with 11 / 34  Batch Loss: 1.175127
=> Done with 12 / 34  Batch Loss: 1.130108
=> Done with 13 / 34  Batch Loss: 1.141151
=> Done with 14 / 34  Batch Loss: 1.186374
=> Done with 15 / 34  Batch Loss: 1.170984
=> Done with 16 / 34  Batch Loss: 1.171685
=> Done with 17 / 34  Batch Loss: 1.231517
=> Done with 18 / 34  Batch Loss: 1.176075
=> Done with 19 / 34  Batch Loss: 1.266937
=> Done with 20 / 34  Batch Loss: 1.143342
=> Done with 21 / 34  Batch Loss: 1.204152
=> Done with 22 / 34  Batch Loss: 1.137830
=> Done with 23 / 34  Batch Loss: 1.175262
=> Done with 24 / 34  Batch Loss: 1.184094
=> Done with 25 / 34  Batch Loss: 1.188205
=> Done with 26 / 34  Batch Loss: 1.184357
=> Done with 27 / 34  Batch Loss: 1.200341
=> Done with 28 / 34  Batch Loss: 1.247039
=> Done with 29 / 34  Batch Loss: 1.177099
=> Done with 30 / 34  Batch Loss: 1.195793
=> Done with 31 / 34  Batch Loss: 1.210012
=> Done with 32 / 34  Batch Loss: 1.152122
=> Done with 33 / 34  Batch Loss: 1.047839
=> Done with 34 / 34  Batch Loss: 1.129513
===> Avg. MSE Loss: 1.179557
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.930067
=> Done with 2 / 34  Batch Loss: 0.911888
=> Done with 3 / 34  Batch Loss: 0.957423
=> Done with 4 / 34  Batch Loss: 0.915842
=> Done with 5 / 34  Batch Loss: 0.926856
=> Done with 6 / 34  Batch Loss: 0.931065
=> Done with 7 / 34  Batch Loss: 0.926128
=> Done with 8 / 34  Batch Loss: 0.927405
=> Done with 9 / 34  Batch Loss: 0.911211
=> Done with 10 / 34  Batch Loss: 0.899054
=> Done with 11 / 34  Batch Loss: 0.909453
=> Done with 12 / 34  Batch Loss: 0.920451
=> Done with 13 / 34  Batch Loss: 0.912454
=> Done with 14 / 34  Batch Loss: 0.923840
=> Done with 15 / 34  Batch Loss: 0.895766
=> Done with 16 / 34  Batch Loss: 0.900546
=> Done with 17 / 34  Batch Loss: 0.922611
=> Done with 18 / 34  Batch Loss: 0.938124
=> Done with 19 / 34  Batch Loss: 0.891471
=> Done with 20 / 34  Batch Loss: 0.918930
=> Done with 21 / 34  Batch Loss: 0.917102
=> Done with 22 / 34  Batch Loss: 0.908523
=> Done with 23 / 34  Batch Loss: 0.929950
=> Done with 24 / 34  Batch Loss: 0.915893
=> Done with 25 / 34  Batch Loss: 0.907126
=> Done with 26 / 34  Batch Loss: 0.884099
=> Done with 27 / 34  Batch Loss: 0.898237
=> Done with 28 / 34  Batch Loss: 0.914433
=> Done with 29 / 34  Batch Loss: 0.915653
=> Done with 30 / 34  Batch Loss: 0.897456
=> Done with 31 / 34  Batch Loss: 0.931600
=> Done with 32 / 34  Batch Loss: 0.888143
=> Done with 33 / 34  Batch Loss: 0.924411
=> Done with 34 / 34  Batch Loss: 0.879775
===> Epoch 8 Complete: Avg. Loss: 0.914206
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.231266
=> Done with 2 / 34  Batch Loss: 1.208370
=> Done with 3 / 34  Batch Loss: 1.220572
=> Done with 4 / 34  Batch Loss: 1.270692
=> Done with 5 / 34  Batch Loss: 1.220519
=> Done with 6 / 34  Batch Loss: 1.213861
=> Done with 7 / 34  Batch Loss: 1.315193
=> Done with 8 / 34  Batch Loss: 1.195832
=> Done with 9 / 34  Batch Loss: 1.243400
=> Done with 10 / 34  Batch Loss: 1.218751
=> Done with 11 / 34  Batch Loss: 1.195044
=> Done with 12 / 34  Batch Loss: 1.278869
=> Done with 13 / 34  Batch Loss: 1.230031
=> Done with 14 / 34  Batch Loss: 1.260666
=> Done with 15 / 34  Batch Loss: 1.275434
=> Done with 16 / 34  Batch Loss: 1.216513
=> Done with 17 / 34  Batch Loss: 1.241807
=> Done with 18 / 34  Batch Loss: 1.264053
=> Done with 19 / 34  Batch Loss: 1.196805
=> Done with 20 / 34  Batch Loss: 1.239906
=> Done with 21 / 34  Batch Loss: 1.184414
=> Done with 22 / 34  Batch Loss: 1.224275
=> Done with 23 / 34  Batch Loss: 1.270003
=> Done with 24 / 34  Batch Loss: 1.259361
=> Done with 25 / 34  Batch Loss: 1.250379
=> Done with 26 / 34  Batch Loss: 1.218026
=> Done with 27 / 34  Batch Loss: 1.224898
=> Done with 28 / 34  Batch Loss: 1.264752
=> Done with 29 / 34  Batch Loss: 1.204428
=> Done with 30 / 34  Batch Loss: 1.187696
=> Done with 31 / 34  Batch Loss: 1.253639
=> Done with 32 / 34  Batch Loss: 1.244850
=> Done with 33 / 34  Batch Loss: 1.198331
=> Done with 34 / 34  Batch Loss: 1.260528
===> Avg. MSE Loss: 1.234799
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.913493
=> Done with 2 / 34  Batch Loss: 0.882010
=> Done with 3 / 34  Batch Loss: 0.887065
=> Done with 4 / 34  Batch Loss: 0.916843
=> Done with 5 / 34  Batch Loss: 0.874632
=> Done with 6 / 34  Batch Loss: 0.889008
=> Done with 7 / 34  Batch Loss: 0.879775
=> Done with 8 / 34  Batch Loss: 0.886080
=> Done with 9 / 34  Batch Loss: 0.889762
=> Done with 10 / 34  Batch Loss: 0.887789
=> Done with 11 / 34  Batch Loss: 0.908190
=> Done with 12 / 34  Batch Loss: 0.876041
=> Done with 13 / 34  Batch Loss: 0.879393
=> Done with 14 / 34  Batch Loss: 0.896471
=> Done with 15 / 34  Batch Loss: 0.883885
=> Done with 16 / 34  Batch Loss: 0.885484
=> Done with 17 / 34  Batch Loss: 0.867284
=> Done with 18 / 34  Batch Loss: 0.866047
=> Done with 19 / 34  Batch Loss: 0.888963
=> Done with 20 / 34  Batch Loss: 0.859048
=> Done with 21 / 34  Batch Loss: 0.863911
=> Done with 22 / 34  Batch Loss: 0.854012
=> Done with 23 / 34  Batch Loss: 0.884918
=> Done with 24 / 34  Batch Loss: 0.866197
=> Done with 25 / 34  Batch Loss: 0.888229
=> Done with 26 / 34  Batch Loss: 0.876731
=> Done with 27 / 34  Batch Loss: 0.865615
=> Done with 28 / 34  Batch Loss: 0.896459
=> Done with 29 / 34  Batch Loss: 0.889648
=> Done with 30 / 34  Batch Loss: 0.873367
=> Done with 31 / 34  Batch Loss: 0.888708
=> Done with 32 / 34  Batch Loss: 0.859409
=> Done with 33 / 34  Batch Loss: 0.874009
=> Done with 34 / 34  Batch Loss: 0.868416
===> Epoch 9 Complete: Avg. Loss: 0.881379
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.310263
=> Done with 2 / 34  Batch Loss: 1.286868
=> Done with 3 / 34  Batch Loss: 1.261019
=> Done with 4 / 34  Batch Loss: 1.255435
=> Done with 5 / 34  Batch Loss: 1.241705
=> Done with 6 / 34  Batch Loss: 1.252708
=> Done with 7 / 34  Batch Loss: 1.342989
=> Done with 8 / 34  Batch Loss: 1.297918
=> Done with 9 / 34  Batch Loss: 1.292236
=> Done with 10 / 34  Batch Loss: 1.298318
=> Done with 11 / 34  Batch Loss: 1.251971
=> Done with 12 / 34  Batch Loss: 1.256557
=> Done with 13 / 34  Batch Loss: 1.204650
=> Done with 14 / 34  Batch Loss: 1.261978
=> Done with 15 / 34  Batch Loss: 1.231826
=> Done with 16 / 34  Batch Loss: 1.326844
=> Done with 17 / 34  Batch Loss: 1.203818
=> Done with 18 / 34  Batch Loss: 1.302575
=> Done with 19 / 34  Batch Loss: 1.286807
=> Done with 20 / 34  Batch Loss: 1.273431
=> Done with 21 / 34  Batch Loss: 1.227200
=> Done with 22 / 34  Batch Loss: 1.297691
=> Done with 23 / 34  Batch Loss: 1.267758
=> Done with 24 / 34  Batch Loss: 1.326496
=> Done with 25 / 34  Batch Loss: 1.249490
=> Done with 26 / 34  Batch Loss: 1.275246
=> Done with 27 / 34  Batch Loss: 1.285598
=> Done with 28 / 34  Batch Loss: 1.295349
=> Done with 29 / 34  Batch Loss: 1.316869
=> Done with 30 / 34  Batch Loss: 1.233746
=> Done with 31 / 34  Batch Loss: 1.318406
=> Done with 32 / 34  Batch Loss: 1.287814
=> Done with 33 / 34  Batch Loss: 1.226420
=> Done with 34 / 34  Batch Loss: 1.211119
===> Avg. MSE Loss: 1.272327
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.861721
=> Done with 2 / 34  Batch Loss: 0.869487
=> Done with 3 / 34  Batch Loss: 0.874943
=> Done with 4 / 34  Batch Loss: 0.854620
=> Done with 5 / 34  Batch Loss: 0.829525
=> Done with 6 / 34  Batch Loss: 0.841690
=> Done with 7 / 34  Batch Loss: 0.853131
=> Done with 8 / 34  Batch Loss: 0.836647
=> Done with 9 / 34  Batch Loss: 0.848526
=> Done with 10 / 34  Batch Loss: 0.832309
=> Done with 11 / 34  Batch Loss: 0.841900
=> Done with 12 / 34  Batch Loss: 0.839939
=> Done with 13 / 34  Batch Loss: 0.833802
=> Done with 14 / 34  Batch Loss: 0.815580
=> Done with 15 / 34  Batch Loss: 0.829816
=> Done with 16 / 34  Batch Loss: 0.822387
=> Done with 17 / 34  Batch Loss: 0.856934
=> Done with 18 / 34  Batch Loss: 0.877885
=> Done with 19 / 34  Batch Loss: 0.869954
=> Done with 20 / 34  Batch Loss: 0.851639
=> Done with 21 / 34  Batch Loss: 0.841496
=> Done with 22 / 34  Batch Loss: 0.833551
=> Done with 23 / 34  Batch Loss: 0.832338
=> Done with 24 / 34  Batch Loss: 0.822678
=> Done with 25 / 34  Batch Loss: 0.836278
=> Done with 26 / 34  Batch Loss: 0.813569
=> Done with 27 / 34  Batch Loss: 0.838505
=> Done with 28 / 34  Batch Loss: 0.851251
=> Done with 29 / 34  Batch Loss: 0.872111
=> Done with 30 / 34  Batch Loss: 0.828402
=> Done with 31 / 34  Batch Loss: 0.814190
=> Done with 32 / 34  Batch Loss: 0.811346
=> Done with 33 / 34  Batch Loss: 0.828151
=> Done with 34 / 34  Batch Loss: 0.821297
===> Epoch 10 Complete: Avg. Loss: 0.840812
===> Checkpoint saved for Epoch 10
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.267113
=> Done with 2 / 34  Batch Loss: 1.216589
=> Done with 3 / 34  Batch Loss: 1.208686
=> Done with 4 / 34  Batch Loss: 1.238757
=> Done with 5 / 34  Batch Loss: 1.158284
=> Done with 6 / 34  Batch Loss: 1.299747
=> Done with 7 / 34  Batch Loss: 1.280733
=> Done with 8 / 34  Batch Loss: 1.230000
=> Done with 9 / 34  Batch Loss: 1.271877
=> Done with 10 / 34  Batch Loss: 1.238875
=> Done with 11 / 34  Batch Loss: 1.245781
=> Done with 12 / 34  Batch Loss: 1.284693
=> Done with 13 / 34  Batch Loss: 1.196448
=> Done with 14 / 34  Batch Loss: 1.278439
=> Done with 15 / 34  Batch Loss: 1.238275
=> Done with 16 / 34  Batch Loss: 1.236677
=> Done with 17 / 34  Batch Loss: 1.235616
=> Done with 18 / 34  Batch Loss: 1.268161
=> Done with 19 / 34  Batch Loss: 1.194088
=> Done with 20 / 34  Batch Loss: 1.276386
=> Done with 21 / 34  Batch Loss: 1.263830
=> Done with 22 / 34  Batch Loss: 1.290149
=> Done with 23 / 34  Batch Loss: 1.255572
=> Done with 24 / 34  Batch Loss: 1.226237
=> Done with 25 / 34  Batch Loss: 1.202523
=> Done with 26 / 34  Batch Loss: 1.282615
=> Done with 27 / 34  Batch Loss: 1.229813
=> Done with 28 / 34  Batch Loss: 1.222228
=> Done with 29 / 34  Batch Loss: 1.277035
=> Done with 30 / 34  Batch Loss: 1.255469
=> Done with 31 / 34  Batch Loss: 1.272788
=> Done with 32 / 34  Batch Loss: 1.248871
=> Done with 33 / 34  Batch Loss: 1.292176
=> Done with 34 / 34  Batch Loss: 1.190059
===> Avg. MSE Loss: 1.246312
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.818262
=> Done with 2 / 34  Batch Loss: 0.810583
=> Done with 3 / 34  Batch Loss: 0.838497
=> Done with 4 / 34  Batch Loss: 0.793746
=> Done with 5 / 34  Batch Loss: 0.833800
=> Done with 6 / 34  Batch Loss: 0.833302
=> Done with 7 / 34  Batch Loss: 0.800214
=> Done with 8 / 34  Batch Loss: 0.826136
=> Done with 9 / 34  Batch Loss: 0.815171
=> Done with 10 / 34  Batch Loss: 0.801851
=> Done with 11 / 34  Batch Loss: 0.826928
=> Done with 12 / 34  Batch Loss: 0.844135
=> Done with 13 / 34  Batch Loss: 0.826191
=> Done with 14 / 34  Batch Loss: 0.803744
=> Done with 15 / 34  Batch Loss: 0.817777
=> Done with 16 / 34  Batch Loss: 0.786065
=> Done with 17 / 34  Batch Loss: 0.806863
=> Done with 18 / 34  Batch Loss: 0.810240
=> Done with 19 / 34  Batch Loss: 0.809490
=> Done with 20 / 34  Batch Loss: 0.811284
=> Done with 21 / 34  Batch Loss: 0.781819
=> Done with 22 / 34  Batch Loss: 0.808975
=> Done with 23 / 34  Batch Loss: 0.796735
=> Done with 24 / 34  Batch Loss: 0.809301
=> Done with 25 / 34  Batch Loss: 0.776982
=> Done with 26 / 34  Batch Loss: 0.814825
=> Done with 27 / 34  Batch Loss: 0.774170
=> Done with 28 / 34  Batch Loss: 0.799735
=> Done with 29 / 34  Batch Loss: 0.809479
=> Done with 30 / 34  Batch Loss: 0.802225
=> Done with 31 / 34  Batch Loss: 0.802772
=> Done with 32 / 34  Batch Loss: 0.759668
=> Done with 33 / 34  Batch Loss: 0.799938
=> Done with 34 / 34  Batch Loss: 0.799466
===> Epoch 11 Complete: Avg. Loss: 0.807364
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.229653
=> Done with 2 / 34  Batch Loss: 1.201102
=> Done with 3 / 34  Batch Loss: 1.168914
=> Done with 4 / 34  Batch Loss: 1.200526
=> Done with 5 / 34  Batch Loss: 1.300539
=> Done with 6 / 34  Batch Loss: 1.130122
=> Done with 7 / 34  Batch Loss: 1.276169
=> Done with 8 / 34  Batch Loss: 1.239974
=> Done with 9 / 34  Batch Loss: 1.242810
=> Done with 10 / 34  Batch Loss: 1.159867
=> Done with 11 / 34  Batch Loss: 1.156291
=> Done with 12 / 34  Batch Loss: 1.263655
=> Done with 13 / 34  Batch Loss: 1.223770
=> Done with 14 / 34  Batch Loss: 1.177377
=> Done with 15 / 34  Batch Loss: 1.221255
=> Done with 16 / 34  Batch Loss: 1.189909
=> Done with 17 / 34  Batch Loss: 1.155877
=> Done with 18 / 34  Batch Loss: 1.204100
=> Done with 19 / 34  Batch Loss: 1.197977
=> Done with 20 / 34  Batch Loss: 1.243424
=> Done with 21 / 34  Batch Loss: 1.216774
=> Done with 22 / 34  Batch Loss: 1.233516
=> Done with 23 / 34  Batch Loss: 1.282434
=> Done with 24 / 34  Batch Loss: 1.167922
=> Done with 25 / 34  Batch Loss: 1.229039
=> Done with 26 / 34  Batch Loss: 1.184224
=> Done with 27 / 34  Batch Loss: 1.194079
=> Done with 28 / 34  Batch Loss: 1.193079
=> Done with 29 / 34  Batch Loss: 1.235480
=> Done with 30 / 34  Batch Loss: 1.275953
=> Done with 31 / 34  Batch Loss: 1.205515
=> Done with 32 / 34  Batch Loss: 1.240459
=> Done with 33 / 34  Batch Loss: 1.168487
=> Done with 34 / 34  Batch Loss: 1.197553
===> Avg. MSE Loss: 1.211995
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.781453
=> Done with 2 / 34  Batch Loss: 0.783525
=> Done with 3 / 34  Batch Loss: 0.754548
=> Done with 4 / 34  Batch Loss: 0.803842
=> Done with 5 / 34  Batch Loss: 0.783023
=> Done with 6 / 34  Batch Loss: 0.787137
=> Done with 7 / 34  Batch Loss: 0.809274
=> Done with 8 / 34  Batch Loss: 0.780774
=> Done with 9 / 34  Batch Loss: 0.818059
=> Done with 10 / 34  Batch Loss: 0.777730
=> Done with 11 / 34  Batch Loss: 0.783317
=> Done with 12 / 34  Batch Loss: 0.786791
=> Done with 13 / 34  Batch Loss: 0.764052
=> Done with 14 / 34  Batch Loss: 0.780276
=> Done with 15 / 34  Batch Loss: 0.837447
=> Done with 16 / 34  Batch Loss: 0.779032
=> Done with 17 / 34  Batch Loss: 0.760407
=> Done with 18 / 34  Batch Loss: 0.802741
=> Done with 19 / 34  Batch Loss: 0.757732
=> Done with 20 / 34  Batch Loss: 0.803026
=> Done with 21 / 34  Batch Loss: 0.786564
=> Done with 22 / 34  Batch Loss: 0.752504
=> Done with 23 / 34  Batch Loss: 0.786090
=> Done with 24 / 34  Batch Loss: 0.756145
=> Done with 25 / 34  Batch Loss: 0.778288
=> Done with 26 / 34  Batch Loss: 0.755068
=> Done with 27 / 34  Batch Loss: 0.791455
=> Done with 28 / 34  Batch Loss: 0.771176
=> Done with 29 / 34  Batch Loss: 0.764729
=> Done with 30 / 34  Batch Loss: 0.757282
=> Done with 31 / 34  Batch Loss: 0.752841
=> Done with 32 / 34  Batch Loss: 0.784419
=> Done with 33 / 34  Batch Loss: 0.738343
=> Done with 34 / 34  Batch Loss: 0.735157
===> Epoch 12 Complete: Avg. Loss: 0.777772
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.292647
=> Done with 2 / 34  Batch Loss: 1.224603
=> Done with 3 / 34  Batch Loss: 1.272551
=> Done with 4 / 34  Batch Loss: 1.273142
=> Done with 5 / 34  Batch Loss: 1.269991
=> Done with 6 / 34  Batch Loss: 1.295940
=> Done with 7 / 34  Batch Loss: 1.263470
=> Done with 8 / 34  Batch Loss: 1.269429
=> Done with 9 / 34  Batch Loss: 1.317133
=> Done with 10 / 34  Batch Loss: 1.258104
=> Done with 11 / 34  Batch Loss: 1.216836
=> Done with 12 / 34  Batch Loss: 1.286989
=> Done with 13 / 34  Batch Loss: 1.222455
=> Done with 14 / 34  Batch Loss: 1.259130
=> Done with 15 / 34  Batch Loss: 1.187894
=> Done with 16 / 34  Batch Loss: 1.288402
=> Done with 17 / 34  Batch Loss: 1.251115
=> Done with 18 / 34  Batch Loss: 1.257868
=> Done with 19 / 34  Batch Loss: 1.215887
=> Done with 20 / 34  Batch Loss: 1.218859
=> Done with 21 / 34  Batch Loss: 1.248017
=> Done with 22 / 34  Batch Loss: 1.311803
=> Done with 23 / 34  Batch Loss: 1.294162
=> Done with 24 / 34  Batch Loss: 1.188958
=> Done with 25 / 34  Batch Loss: 1.282540
=> Done with 26 / 34  Batch Loss: 1.225958
=> Done with 27 / 34  Batch Loss: 1.270667
=> Done with 28 / 34  Batch Loss: 1.245259
=> Done with 29 / 34  Batch Loss: 1.266074
=> Done with 30 / 34  Batch Loss: 1.245152
=> Done with 31 / 34  Batch Loss: 1.184342
=> Done with 32 / 34  Batch Loss: 1.258119
=> Done with 33 / 34  Batch Loss: 1.167472
=> Done with 34 / 34  Batch Loss: 1.291549
===> Avg. MSE Loss: 1.253604
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.742128
=> Done with 2 / 34  Batch Loss: 0.764295
=> Done with 3 / 34  Batch Loss: 0.739762
=> Done with 4 / 34  Batch Loss: 0.736758
=> Done with 5 / 34  Batch Loss: 0.733564
=> Done with 6 / 34  Batch Loss: 0.752941
=> Done with 7 / 34  Batch Loss: 0.752799
=> Done with 8 / 34  Batch Loss: 0.732655
=> Done with 9 / 34  Batch Loss: 0.751564
=> Done with 10 / 34  Batch Loss: 0.720920
=> Done with 11 / 34  Batch Loss: 0.744827
=> Done with 12 / 34  Batch Loss: 0.749115
=> Done with 13 / 34  Batch Loss: 0.729793
=> Done with 14 / 34  Batch Loss: 0.762763
=> Done with 15 / 34  Batch Loss: 0.738765
=> Done with 16 / 34  Batch Loss: 0.721988
=> Done with 17 / 34  Batch Loss: 0.718820
=> Done with 18 / 34  Batch Loss: 0.736395
=> Done with 19 / 34  Batch Loss: 0.743311
=> Done with 20 / 34  Batch Loss: 0.749832
=> Done with 21 / 34  Batch Loss: 0.744222
=> Done with 22 / 34  Batch Loss: 0.726181
=> Done with 23 / 34  Batch Loss: 0.764362
=> Done with 24 / 34  Batch Loss: 0.747968
=> Done with 25 / 34  Batch Loss: 0.729794
=> Done with 26 / 34  Batch Loss: 0.717424
=> Done with 27 / 34  Batch Loss: 0.739624
=> Done with 28 / 34  Batch Loss: 0.717959
=> Done with 29 / 34  Batch Loss: 0.719666
=> Done with 30 / 34  Batch Loss: 0.739797
=> Done with 31 / 34  Batch Loss: 0.747560
=> Done with 32 / 34  Batch Loss: 0.749142
=> Done with 33 / 34  Batch Loss: 0.734300
=> Done with 34 / 34  Batch Loss: 0.758508
===> Epoch 13 Complete: Avg. Loss: 0.739985
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.273090
=> Done with 2 / 34  Batch Loss: 1.188193
=> Done with 3 / 34  Batch Loss: 1.269037
=> Done with 4 / 34  Batch Loss: 1.307205
=> Done with 5 / 34  Batch Loss: 1.203516
=> Done with 6 / 34  Batch Loss: 1.293709
=> Done with 7 / 34  Batch Loss: 1.241636
=> Done with 8 / 34  Batch Loss: 1.238081
=> Done with 9 / 34  Batch Loss: 1.271678
=> Done with 10 / 34  Batch Loss: 1.231220
=> Done with 11 / 34  Batch Loss: 1.309759
=> Done with 12 / 34  Batch Loss: 1.219864
=> Done with 13 / 34  Batch Loss: 1.234763
=> Done with 14 / 34  Batch Loss: 1.213794
=> Done with 15 / 34  Batch Loss: 1.223138
=> Done with 16 / 34  Batch Loss: 1.343558
=> Done with 17 / 34  Batch Loss: 1.268417
=> Done with 18 / 34  Batch Loss: 1.245772
=> Done with 19 / 34  Batch Loss: 1.260713
=> Done with 20 / 34  Batch Loss: 1.264501
=> Done with 21 / 34  Batch Loss: 1.302835
=> Done with 22 / 34  Batch Loss: 1.325782
=> Done with 23 / 34  Batch Loss: 1.226678
=> Done with 24 / 34  Batch Loss: 1.241905
=> Done with 25 / 34  Batch Loss: 1.217459
=> Done with 26 / 34  Batch Loss: 1.340183
=> Done with 27 / 34  Batch Loss: 1.254781
=> Done with 28 / 34  Batch Loss: 1.244811
=> Done with 29 / 34  Batch Loss: 1.242798
=> Done with 30 / 34  Batch Loss: 1.244284
=> Done with 31 / 34  Batch Loss: 1.225177
=> Done with 32 / 34  Batch Loss: 1.250236
=> Done with 33 / 34  Batch Loss: 1.295092
=> Done with 34 / 34  Batch Loss: 1.154874
===> Avg. MSE Loss: 1.254957
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.742549
=> Done with 2 / 34  Batch Loss: 0.750441
=> Done with 3 / 34  Batch Loss: 0.744925
=> Done with 4 / 34  Batch Loss: 0.735460
=> Done with 5 / 34  Batch Loss: 0.715591
=> Done with 6 / 34  Batch Loss: 0.736994
=> Done with 7 / 34  Batch Loss: 0.724898
=> Done with 8 / 34  Batch Loss: 0.742395
=> Done with 9 / 34  Batch Loss: 0.737238
=> Done with 10 / 34  Batch Loss: 0.751247
=> Done with 11 / 34  Batch Loss: 0.711083
=> Done with 12 / 34  Batch Loss: 0.725857
=> Done with 13 / 34  Batch Loss: 0.708643
=> Done with 14 / 34  Batch Loss: 0.738610
=> Done with 15 / 34  Batch Loss: 0.694533
=> Done with 16 / 34  Batch Loss: 0.736875
=> Done with 17 / 34  Batch Loss: 0.714939
=> Done with 18 / 34  Batch Loss: 0.758571
=> Done with 19 / 34  Batch Loss: 0.715246
=> Done with 20 / 34  Batch Loss: 0.704537
=> Done with 21 / 34  Batch Loss: 0.710327
=> Done with 22 / 34  Batch Loss: 0.708518
=> Done with 23 / 34  Batch Loss: 0.718002
=> Done with 24 / 34  Batch Loss: 0.710008
=> Done with 25 / 34  Batch Loss: 0.730779
=> Done with 26 / 34  Batch Loss: 0.734204
=> Done with 27 / 34  Batch Loss: 0.727382
=> Done with 28 / 34  Batch Loss: 0.709710
=> Done with 29 / 34  Batch Loss: 0.730446
=> Done with 30 / 34  Batch Loss: 0.707111
=> Done with 31 / 34  Batch Loss: 0.711948
=> Done with 32 / 34  Batch Loss: 0.691498
=> Done with 33 / 34  Batch Loss: 0.691122
=> Done with 34 / 34  Batch Loss: 0.701062
===> Epoch 14 Complete: Avg. Loss: 0.722728
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.207501
=> Done with 2 / 34  Batch Loss: 1.217912
=> Done with 3 / 34  Batch Loss: 1.167768
=> Done with 4 / 34  Batch Loss: 1.218278
=> Done with 5 / 34  Batch Loss: 1.279201
=> Done with 6 / 34  Batch Loss: 1.141590
=> Done with 7 / 34  Batch Loss: 1.131734
=> Done with 8 / 34  Batch Loss: 1.227332
=> Done with 9 / 34  Batch Loss: 1.216258
=> Done with 10 / 34  Batch Loss: 1.237664
=> Done with 11 / 34  Batch Loss: 1.210106
=> Done with 12 / 34  Batch Loss: 1.164546
=> Done with 13 / 34  Batch Loss: 1.204677
=> Done with 14 / 34  Batch Loss: 1.192447
=> Done with 15 / 34  Batch Loss: 1.234651
=> Done with 16 / 34  Batch Loss: 1.242291
=> Done with 17 / 34  Batch Loss: 1.222471
=> Done with 18 / 34  Batch Loss: 1.229146
=> Done with 19 / 34  Batch Loss: 1.198184
=> Done with 20 / 34  Batch Loss: 1.202528
=> Done with 21 / 34  Batch Loss: 1.179820
=> Done with 22 / 34  Batch Loss: 1.131923
=> Done with 23 / 34  Batch Loss: 1.206610
=> Done with 24 / 34  Batch Loss: 1.212811
=> Done with 25 / 34  Batch Loss: 1.151437
=> Done with 26 / 34  Batch Loss: 1.232714
=> Done with 27 / 34  Batch Loss: 1.256954
=> Done with 28 / 34  Batch Loss: 1.256085
=> Done with 29 / 34  Batch Loss: 1.214717
=> Done with 30 / 34  Batch Loss: 1.215837
=> Done with 31 / 34  Batch Loss: 1.211178
=> Done with 32 / 34  Batch Loss: 1.184911
=> Done with 33 / 34  Batch Loss: 1.250357
=> Done with 34 / 34  Batch Loss: 1.172390
===> Avg. MSE Loss: 1.206589
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.685661
=> Done with 2 / 34  Batch Loss: 0.725268
=> Done with 3 / 34  Batch Loss: 0.692317
=> Done with 4 / 34  Batch Loss: 0.702425
=> Done with 5 / 34  Batch Loss: 0.689936
=> Done with 6 / 34  Batch Loss: 0.718684
=> Done with 7 / 34  Batch Loss: 0.681188
=> Done with 8 / 34  Batch Loss: 0.718159
=> Done with 9 / 34  Batch Loss: 0.688053
=> Done with 10 / 34  Batch Loss: 0.702047
=> Done with 11 / 34  Batch Loss: 0.692602
=> Done with 12 / 34  Batch Loss: 0.706041
=> Done with 13 / 34  Batch Loss: 0.691913
=> Done with 14 / 34  Batch Loss: 0.676273
=> Done with 15 / 34  Batch Loss: 0.681249
=> Done with 16 / 34  Batch Loss: 0.693639
=> Done with 17 / 34  Batch Loss: 0.679509
=> Done with 18 / 34  Batch Loss: 0.688474
=> Done with 19 / 34  Batch Loss: 0.682521
=> Done with 20 / 34  Batch Loss: 0.717596
=> Done with 21 / 34  Batch Loss: 0.702689
=> Done with 22 / 34  Batch Loss: 0.673116
=> Done with 23 / 34  Batch Loss: 0.681847
=> Done with 24 / 34  Batch Loss: 0.686247
=> Done with 25 / 34  Batch Loss: 0.707795
=> Done with 26 / 34  Batch Loss: 0.686110
=> Done with 27 / 34  Batch Loss: 0.676713
=> Done with 28 / 34  Batch Loss: 0.647175
=> Done with 29 / 34  Batch Loss: 0.701255
=> Done with 30 / 34  Batch Loss: 0.679285
=> Done with 31 / 34  Batch Loss: 0.655710
=> Done with 32 / 34  Batch Loss: 0.689748
=> Done with 33 / 34  Batch Loss: 0.703868
=> Done with 34 / 34  Batch Loss: 0.684574
===> Epoch 15 Complete: Avg. Loss: 0.690873
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.260990
=> Done with 2 / 34  Batch Loss: 1.238454
=> Done with 3 / 34  Batch Loss: 1.215885
=> Done with 4 / 34  Batch Loss: 1.218653
=> Done with 5 / 34  Batch Loss: 1.268826
=> Done with 6 / 34  Batch Loss: 1.212826
=> Done with 7 / 34  Batch Loss: 1.176856
=> Done with 8 / 34  Batch Loss: 1.252914
=> Done with 9 / 34  Batch Loss: 1.205542
=> Done with 10 / 34  Batch Loss: 1.279663
=> Done with 11 / 34  Batch Loss: 1.283752
=> Done with 12 / 34  Batch Loss: 1.241635
=> Done with 13 / 34  Batch Loss: 1.270820
=> Done with 14 / 34  Batch Loss: 1.169891
=> Done with 15 / 34  Batch Loss: 1.237364
=> Done with 16 / 34  Batch Loss: 1.246893
=> Done with 17 / 34  Batch Loss: 1.287598
=> Done with 18 / 34  Batch Loss: 1.291183
=> Done with 19 / 34  Batch Loss: 1.278209
=> Done with 20 / 34  Batch Loss: 1.217983
=> Done with 21 / 34  Batch Loss: 1.235442
=> Done with 22 / 34  Batch Loss: 1.254924
=> Done with 23 / 34  Batch Loss: 1.271158
=> Done with 24 / 34  Batch Loss: 1.227109
=> Done with 25 / 34  Batch Loss: 1.219354
=> Done with 26 / 34  Batch Loss: 1.238423
=> Done with 27 / 34  Batch Loss: 1.194399
=> Done with 28 / 34  Batch Loss: 1.415027
=> Done with 29 / 34  Batch Loss: 1.241454
=> Done with 30 / 34  Batch Loss: 1.297278
=> Done with 31 / 34  Batch Loss: 1.308445
=> Done with 32 / 34  Batch Loss: 1.233574
=> Done with 33 / 34  Batch Loss: 1.215118
=> Done with 34 / 34  Batch Loss: 1.261386
===> Avg. MSE Loss: 1.249089
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.676766
=> Done with 2 / 34  Batch Loss: 0.679335
=> Done with 3 / 34  Batch Loss: 0.655130
=> Done with 4 / 34  Batch Loss: 0.628607
=> Done with 5 / 34  Batch Loss: 0.671970
=> Done with 6 / 34  Batch Loss: 0.688267
=> Done with 7 / 34  Batch Loss: 0.702198
=> Done with 8 / 34  Batch Loss: 0.650919
=> Done with 9 / 34  Batch Loss: 0.690317
=> Done with 10 / 34  Batch Loss: 0.652161
=> Done with 11 / 34  Batch Loss: 0.672761
=> Done with 12 / 34  Batch Loss: 0.666764
=> Done with 13 / 34  Batch Loss: 0.672969
=> Done with 14 / 34  Batch Loss: 0.665390
=> Done with 15 / 34  Batch Loss: 0.651455
=> Done with 16 / 34  Batch Loss: 0.670314
=> Done with 17 / 34  Batch Loss: 0.642856
=> Done with 18 / 34  Batch Loss: 0.661462
=> Done with 19 / 34  Batch Loss: 0.637412
=> Done with 20 / 34  Batch Loss: 0.665917
=> Done with 21 / 34  Batch Loss: 0.677314
=> Done with 22 / 34  Batch Loss: 0.672116
=> Done with 23 / 34  Batch Loss: 0.647336
=> Done with 24 / 34  Batch Loss: 0.662840
=> Done with 25 / 34  Batch Loss: 0.628368
=> Done with 26 / 34  Batch Loss: 0.651960
=> Done with 27 / 34  Batch Loss: 0.685687
=> Done with 28 / 34  Batch Loss: 0.633401
=> Done with 29 / 34  Batch Loss: 0.658569
=> Done with 30 / 34  Batch Loss: 0.630023
=> Done with 31 / 34  Batch Loss: 0.663944
=> Done with 32 / 34  Batch Loss: 0.643372
=> Done with 33 / 34  Batch Loss: 0.667245
=> Done with 34 / 34  Batch Loss: 0.662745
===> Epoch 16 Complete: Avg. Loss: 0.661408
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.243699
=> Done with 2 / 34  Batch Loss: 1.240574
=> Done with 3 / 34  Batch Loss: 1.272265
=> Done with 4 / 34  Batch Loss: 1.218755
=> Done with 5 / 34  Batch Loss: 1.316949
=> Done with 6 / 34  Batch Loss: 1.264719
=> Done with 7 / 34  Batch Loss: 1.249325
=> Done with 8 / 34  Batch Loss: 1.248648
=> Done with 9 / 34  Batch Loss: 1.233561
=> Done with 10 / 34  Batch Loss: 1.177768
=> Done with 11 / 34  Batch Loss: 1.248644
=> Done with 12 / 34  Batch Loss: 1.153368
=> Done with 13 / 34  Batch Loss: 1.327395
=> Done with 14 / 34  Batch Loss: 1.245080
=> Done with 15 / 34  Batch Loss: 1.285783
=> Done with 16 / 34  Batch Loss: 1.227049
=> Done with 17 / 34  Batch Loss: 1.319382
=> Done with 18 / 34  Batch Loss: 1.294059
=> Done with 19 / 34  Batch Loss: 1.249614
=> Done with 20 / 34  Batch Loss: 1.246674
=> Done with 21 / 34  Batch Loss: 1.272764
=> Done with 22 / 34  Batch Loss: 1.280522
=> Done with 23 / 34  Batch Loss: 1.244634
=> Done with 24 / 34  Batch Loss: 1.213524
=> Done with 25 / 34  Batch Loss: 1.285966
=> Done with 26 / 34  Batch Loss: 1.302324
=> Done with 27 / 34  Batch Loss: 1.160732
=> Done with 28 / 34  Batch Loss: 1.350814
=> Done with 29 / 34  Batch Loss: 1.211921
=> Done with 30 / 34  Batch Loss: 1.251216
=> Done with 31 / 34  Batch Loss: 1.278641
=> Done with 32 / 34  Batch Loss: 1.228387
=> Done with 33 / 34  Batch Loss: 1.239897
=> Done with 34 / 34  Batch Loss: 1.145202
===> Avg. MSE Loss: 1.250878
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.653841
=> Done with 2 / 34  Batch Loss: 0.644435
=> Done with 3 / 34  Batch Loss: 0.649071
=> Done with 4 / 34  Batch Loss: 0.654239
=> Done with 5 / 34  Batch Loss: 0.677731
=> Done with 6 / 34  Batch Loss: 0.653051
=> Done with 7 / 34  Batch Loss: 0.653690
=> Done with 8 / 34  Batch Loss: 0.655090
=> Done with 9 / 34  Batch Loss: 0.627566
=> Done with 10 / 34  Batch Loss: 0.643580
=> Done with 11 / 34  Batch Loss: 0.632505
=> Done with 12 / 34  Batch Loss: 0.669203
=> Done with 13 / 34  Batch Loss: 0.640222
=> Done with 14 / 34  Batch Loss: 0.671921
=> Done with 15 / 34  Batch Loss: 0.635081
=> Done with 16 / 34  Batch Loss: 0.647933
=> Done with 17 / 34  Batch Loss: 0.635566
=> Done with 18 / 34  Batch Loss: 0.676349
=> Done with 19 / 34  Batch Loss: 0.666275
=> Done with 20 / 34  Batch Loss: 0.646025
=> Done with 21 / 34  Batch Loss: 0.646127
=> Done with 22 / 34  Batch Loss: 0.633206
=> Done with 23 / 34  Batch Loss: 0.636185
=> Done with 24 / 34  Batch Loss: 0.688651
=> Done with 25 / 34  Batch Loss: 0.630853
=> Done with 26 / 34  Batch Loss: 0.661607
=> Done with 27 / 34  Batch Loss: 0.621339
=> Done with 28 / 34  Batch Loss: 0.633352
=> Done with 29 / 34  Batch Loss: 0.637519
=> Done with 30 / 34  Batch Loss: 0.635951
=> Done with 31 / 34  Batch Loss: 0.614095
=> Done with 32 / 34  Batch Loss: 0.626981
=> Done with 33 / 34  Batch Loss: 0.638075
=> Done with 34 / 34  Batch Loss: 0.623793
===> Epoch 17 Complete: Avg. Loss: 0.645915
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.287673
=> Done with 2 / 34  Batch Loss: 1.252458
=> Done with 3 / 34  Batch Loss: 1.218017
=> Done with 4 / 34  Batch Loss: 1.263074
=> Done with 5 / 34  Batch Loss: 1.265562
=> Done with 6 / 34  Batch Loss: 1.299782
=> Done with 7 / 34  Batch Loss: 1.246317
=> Done with 8 / 34  Batch Loss: 1.354277
=> Done with 9 / 34  Batch Loss: 1.275316
=> Done with 10 / 34  Batch Loss: 1.258835
=> Done with 11 / 34  Batch Loss: 1.333673
=> Done with 12 / 34  Batch Loss: 1.170919
=> Done with 13 / 34  Batch Loss: 1.344395
=> Done with 14 / 34  Batch Loss: 1.264442
=> Done with 15 / 34  Batch Loss: 1.271000
=> Done with 16 / 34  Batch Loss: 1.260869
=> Done with 17 / 34  Batch Loss: 1.223256
=> Done with 18 / 34  Batch Loss: 1.160001
=> Done with 19 / 34  Batch Loss: 1.279335
=> Done with 20 / 34  Batch Loss: 1.260882
=> Done with 21 / 34  Batch Loss: 1.246840
=> Done with 22 / 34  Batch Loss: 1.181548
=> Done with 23 / 34  Batch Loss: 1.243215
=> Done with 24 / 34  Batch Loss: 1.225803
=> Done with 25 / 34  Batch Loss: 1.213824
=> Done with 26 / 34  Batch Loss: 1.306445
=> Done with 27 / 34  Batch Loss: 1.188616
=> Done with 28 / 34  Batch Loss: 1.172790
=> Done with 29 / 34  Batch Loss: 1.282353
=> Done with 30 / 34  Batch Loss: 1.303700
=> Done with 31 / 34  Batch Loss: 1.295751
=> Done with 32 / 34  Batch Loss: 1.254835
=> Done with 33 / 34  Batch Loss: 1.302252
=> Done with 34 / 34  Batch Loss: 1.226267
===> Avg. MSE Loss: 1.256892
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.634390
=> Done with 2 / 34  Batch Loss: 0.639851
=> Done with 3 / 34  Batch Loss: 0.650243
=> Done with 4 / 34  Batch Loss: 0.619421
=> Done with 5 / 34  Batch Loss: 0.625776
=> Done with 6 / 34  Batch Loss: 0.621293
=> Done with 7 / 34  Batch Loss: 0.625883
=> Done with 8 / 34  Batch Loss: 0.603419
=> Done with 9 / 34  Batch Loss: 0.648275
=> Done with 10 / 34  Batch Loss: 0.620789
=> Done with 11 / 34  Batch Loss: 0.596497
=> Done with 12 / 34  Batch Loss: 0.636610
=> Done with 13 / 34  Batch Loss: 0.641785
=> Done with 14 / 34  Batch Loss: 0.624080
=> Done with 15 / 34  Batch Loss: 0.604982
=> Done with 16 / 34  Batch Loss: 0.598030
=> Done with 17 / 34  Batch Loss: 0.647605
=> Done with 18 / 34  Batch Loss: 0.619128
=> Done with 19 / 34  Batch Loss: 0.624174
=> Done with 20 / 34  Batch Loss: 0.619741
=> Done with 21 / 34  Batch Loss: 0.619554
=> Done with 22 / 34  Batch Loss: 0.637068
=> Done with 23 / 34  Batch Loss: 0.619116
=> Done with 24 / 34  Batch Loss: 0.607545
=> Done with 25 / 34  Batch Loss: 0.595402
=> Done with 26 / 34  Batch Loss: 0.593659
=> Done with 27 / 34  Batch Loss: 0.611994
=> Done with 28 / 34  Batch Loss: 0.617261
=> Done with 29 / 34  Batch Loss: 0.591776
=> Done with 30 / 34  Batch Loss: 0.627451
=> Done with 31 / 34  Batch Loss: 0.608449
=> Done with 32 / 34  Batch Loss: 0.613708
=> Done with 33 / 34  Batch Loss: 0.592903
=> Done with 34 / 34  Batch Loss: 0.612521
===> Epoch 18 Complete: Avg. Loss: 0.619129
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.210545
=> Done with 2 / 34  Batch Loss: 1.228250
=> Done with 3 / 34  Batch Loss: 1.269702
=> Done with 4 / 34  Batch Loss: 1.195427
=> Done with 5 / 34  Batch Loss: 1.315329
=> Done with 6 / 34  Batch Loss: 1.287836
=> Done with 7 / 34  Batch Loss: 1.179028
=> Done with 8 / 34  Batch Loss: 1.279451
=> Done with 9 / 34  Batch Loss: 1.166935
=> Done with 10 / 34  Batch Loss: 1.296157
=> Done with 11 / 34  Batch Loss: 1.239512
=> Done with 12 / 34  Batch Loss: 1.196818
=> Done with 13 / 34  Batch Loss: 1.148331
=> Done with 14 / 34  Batch Loss: 1.226990
=> Done with 15 / 34  Batch Loss: 1.242442
=> Done with 16 / 34  Batch Loss: 1.197893
=> Done with 17 / 34  Batch Loss: 1.261075
=> Done with 18 / 34  Batch Loss: 1.169528
=> Done with 19 / 34  Batch Loss: 1.247726
=> Done with 20 / 34  Batch Loss: 1.247620
=> Done with 21 / 34  Batch Loss: 1.274319
=> Done with 22 / 34  Batch Loss: 1.270720
=> Done with 23 / 34  Batch Loss: 1.201531
=> Done with 24 / 34  Batch Loss: 1.247758
=> Done with 25 / 34  Batch Loss: 1.230975
=> Done with 26 / 34  Batch Loss: 1.204037
=> Done with 27 / 34  Batch Loss: 1.238994
=> Done with 28 / 34  Batch Loss: 1.210966
=> Done with 29 / 34  Batch Loss: 1.126619
=> Done with 30 / 34  Batch Loss: 1.176010
=> Done with 31 / 34  Batch Loss: 1.194350
=> Done with 32 / 34  Batch Loss: 1.212849
=> Done with 33 / 34  Batch Loss: 1.310010
=> Done with 34 / 34  Batch Loss: 1.202678
===> Avg. MSE Loss: 1.226718
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.620391
=> Done with 2 / 34  Batch Loss: 0.625506
=> Done with 3 / 34  Batch Loss: 0.600030
=> Done with 4 / 34  Batch Loss: 0.591474
=> Done with 5 / 34  Batch Loss: 0.587873
=> Done with 6 / 34  Batch Loss: 0.581725
=> Done with 7 / 34  Batch Loss: 0.624679
=> Done with 8 / 34  Batch Loss: 0.629481
=> Done with 9 / 34  Batch Loss: 0.577660
=> Done with 10 / 34  Batch Loss: 0.609356
=> Done with 11 / 34  Batch Loss: 0.597472
=> Done with 12 / 34  Batch Loss: 0.578666
=> Done with 13 / 34  Batch Loss: 0.588066
=> Done with 14 / 34  Batch Loss: 0.598588
=> Done with 15 / 34  Batch Loss: 0.591704
=> Done with 16 / 34  Batch Loss: 0.604376
=> Done with 17 / 34  Batch Loss: 0.638808
=> Done with 18 / 34  Batch Loss: 0.603216
=> Done with 19 / 34  Batch Loss: 0.583083
=> Done with 20 / 34  Batch Loss: 0.626396
=> Done with 21 / 34  Batch Loss: 0.564355
=> Done with 22 / 34  Batch Loss: 0.584351
=> Done with 23 / 34  Batch Loss: 0.592738
=> Done with 24 / 34  Batch Loss: 0.586260
=> Done with 25 / 34  Batch Loss: 0.625274
=> Done with 26 / 34  Batch Loss: 0.589637
=> Done with 27 / 34  Batch Loss: 0.577532
=> Done with 28 / 34  Batch Loss: 0.613670
=> Done with 29 / 34  Batch Loss: 0.598601
=> Done with 30 / 34  Batch Loss: 0.581462
=> Done with 31 / 34  Batch Loss: 0.619912
=> Done with 32 / 34  Batch Loss: 0.613025
=> Done with 33 / 34  Batch Loss: 0.597354
=> Done with 34 / 34  Batch Loss: 0.582843
===> Epoch 19 Complete: Avg. Loss: 0.599575
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.313367
=> Done with 2 / 34  Batch Loss: 1.329612
=> Done with 3 / 34  Batch Loss: 1.260354
=> Done with 4 / 34  Batch Loss: 1.326522
=> Done with 5 / 34  Batch Loss: 1.250118
=> Done with 6 / 34  Batch Loss: 1.339889
=> Done with 7 / 34  Batch Loss: 1.303507
=> Done with 8 / 34  Batch Loss: 1.229172
=> Done with 9 / 34  Batch Loss: 1.344963
=> Done with 10 / 34  Batch Loss: 1.331784
=> Done with 11 / 34  Batch Loss: 1.298938
=> Done with 12 / 34  Batch Loss: 1.286208
=> Done with 13 / 34  Batch Loss: 1.278114
=> Done with 14 / 34  Batch Loss: 1.283342
=> Done with 15 / 34  Batch Loss: 1.308517
=> Done with 16 / 34  Batch Loss: 1.237754
=> Done with 17 / 34  Batch Loss: 1.274240
=> Done with 18 / 34  Batch Loss: 1.274365
=> Done with 19 / 34  Batch Loss: 1.356383
=> Done with 20 / 34  Batch Loss: 1.349067
=> Done with 21 / 34  Batch Loss: 1.326729
=> Done with 22 / 34  Batch Loss: 1.271811
=> Done with 23 / 34  Batch Loss: 1.285001
=> Done with 24 / 34  Batch Loss: 1.176115
=> Done with 25 / 34  Batch Loss: 1.283387
=> Done with 26 / 34  Batch Loss: 1.251736
=> Done with 27 / 34  Batch Loss: 1.328898
=> Done with 28 / 34  Batch Loss: 1.299530
=> Done with 29 / 34  Batch Loss: 1.346402
=> Done with 30 / 34  Batch Loss: 1.273687
=> Done with 31 / 34  Batch Loss: 1.328032
=> Done with 32 / 34  Batch Loss: 1.256280
=> Done with 33 / 34  Batch Loss: 1.346374
=> Done with 34 / 34  Batch Loss: 1.214313
===> Avg. MSE Loss: 1.293074
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.603226
=> Done with 2 / 34  Batch Loss: 0.612190
=> Done with 3 / 34  Batch Loss: 0.618547
=> Done with 4 / 34  Batch Loss: 0.561441
=> Done with 5 / 34  Batch Loss: 0.583695
=> Done with 6 / 34  Batch Loss: 0.618531
=> Done with 7 / 34  Batch Loss: 0.590176
=> Done with 8 / 34  Batch Loss: 0.596691
=> Done with 9 / 34  Batch Loss: 0.624944
=> Done with 10 / 34  Batch Loss: 0.577742
=> Done with 11 / 34  Batch Loss: 0.619797
=> Done with 12 / 34  Batch Loss: 0.584139
=> Done with 13 / 34  Batch Loss: 0.588548
=> Done with 14 / 34  Batch Loss: 0.590286
=> Done with 15 / 34  Batch Loss: 0.594991
=> Done with 16 / 34  Batch Loss: 0.585011
=> Done with 17 / 34  Batch Loss: 0.586377
=> Done with 18 / 34  Batch Loss: 0.589491
=> Done with 19 / 34  Batch Loss: 0.545982
=> Done with 20 / 34  Batch Loss: 0.561698
=> Done with 21 / 34  Batch Loss: 0.615707
=> Done with 22 / 34  Batch Loss: 0.560045
=> Done with 23 / 34  Batch Loss: 0.533390
=> Done with 24 / 34  Batch Loss: 0.586388
=> Done with 25 / 34  Batch Loss: 0.593658
=> Done with 26 / 34  Batch Loss: 0.546503
=> Done with 27 / 34  Batch Loss: 0.576787
=> Done with 28 / 34  Batch Loss: 0.598162
=> Done with 29 / 34  Batch Loss: 0.561653
=> Done with 30 / 34  Batch Loss: 0.574287
=> Done with 31 / 34  Batch Loss: 0.591254
=> Done with 32 / 34  Batch Loss: 0.560627
=> Done with 33 / 34  Batch Loss: 0.531165
=> Done with 34 / 34  Batch Loss: 0.580949
===> Epoch 20 Complete: Avg. Loss: 0.583649
===> Checkpoint saved for Epoch 20
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.343201
=> Done with 2 / 34  Batch Loss: 1.297853
=> Done with 3 / 34  Batch Loss: 1.330666
=> Done with 4 / 34  Batch Loss: 1.307498
=> Done with 5 / 34  Batch Loss: 1.406290
=> Done with 6 / 34  Batch Loss: 1.331751
=> Done with 7 / 34  Batch Loss: 1.399870
=> Done with 8 / 34  Batch Loss: 1.407688
=> Done with 9 / 34  Batch Loss: 1.402597
=> Done with 10 / 34  Batch Loss: 1.371905
=> Done with 11 / 34  Batch Loss: 1.412919
=> Done with 12 / 34  Batch Loss: 1.404211
=> Done with 13 / 34  Batch Loss: 1.311924
=> Done with 14 / 34  Batch Loss: 1.318481
=> Done with 15 / 34  Batch Loss: 1.428159
=> Done with 16 / 34  Batch Loss: 1.307584
=> Done with 17 / 34  Batch Loss: 1.188650
=> Done with 18 / 34  Batch Loss: 1.321129
=> Done with 19 / 34  Batch Loss: 1.408840
=> Done with 20 / 34  Batch Loss: 1.424927
=> Done with 21 / 34  Batch Loss: 1.424613
=> Done with 22 / 34  Batch Loss: 1.301840
=> Done with 23 / 34  Batch Loss: 1.304607
=> Done with 24 / 34  Batch Loss: 1.233073
=> Done with 25 / 34  Batch Loss: 1.265499
=> Done with 26 / 34  Batch Loss: 1.397940
=> Done with 27 / 34  Batch Loss: 1.391781
=> Done with 28 / 34  Batch Loss: 1.260802
=> Done with 29 / 34  Batch Loss: 1.348077
=> Done with 30 / 34  Batch Loss: 1.336535
=> Done with 31 / 34  Batch Loss: 1.305234
=> Done with 32 / 34  Batch Loss: 1.291674
=> Done with 33 / 34  Batch Loss: 1.336867
=> Done with 34 / 34  Batch Loss: 1.274558
===> Avg. MSE Loss: 1.341154
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.559435
=> Done with 2 / 34  Batch Loss: 0.560573
=> Done with 3 / 34  Batch Loss: 0.661296
=> Done with 4 / 34  Batch Loss: 0.641305
=> Done with 5 / 34  Batch Loss: 0.539048
=> Done with 6 / 34  Batch Loss: 0.551264
=> Done with 7 / 34  Batch Loss: 0.546222
=> Done with 8 / 34  Batch Loss: 0.563219
=> Done with 9 / 34  Batch Loss: 0.540286
=> Done with 10 / 34  Batch Loss: 0.552168
=> Done with 11 / 34  Batch Loss: 0.557772
=> Done with 12 / 34  Batch Loss: 0.562952
=> Done with 13 / 34  Batch Loss: 0.562258
=> Done with 14 / 34  Batch Loss: 0.604574
=> Done with 15 / 34  Batch Loss: 0.517967
=> Done with 16 / 34  Batch Loss: 0.572270
=> Done with 17 / 34  Batch Loss: 0.576909
=> Done with 18 / 34  Batch Loss: 0.535131
=> Done with 19 / 34  Batch Loss: 0.557774
=> Done with 20 / 34  Batch Loss: 0.563346
=> Done with 21 / 34  Batch Loss: 0.556480
=> Done with 22 / 34  Batch Loss: 0.596427
=> Done with 23 / 34  Batch Loss: 0.559382
=> Done with 24 / 34  Batch Loss: 0.584438
=> Done with 25 / 34  Batch Loss: 0.567567
=> Done with 26 / 34  Batch Loss: 0.554659
=> Done with 27 / 34  Batch Loss: 0.540857
=> Done with 28 / 34  Batch Loss: 0.530620
=> Done with 29 / 34  Batch Loss: 0.567883
=> Done with 30 / 34  Batch Loss: 0.569224
=> Done with 31 / 34  Batch Loss: 0.554692
=> Done with 32 / 34  Batch Loss: 0.552374
=> Done with 33 / 34  Batch Loss: 0.564909
=> Done with 34 / 34  Batch Loss: 0.541234
===> Epoch 21 Complete: Avg. Loss: 0.563721
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.289834
=> Done with 2 / 34  Batch Loss: 1.280390
=> Done with 3 / 34  Batch Loss: 1.358663
=> Done with 4 / 34  Batch Loss: 1.241963
=> Done with 5 / 34  Batch Loss: 1.248442
=> Done with 6 / 34  Batch Loss: 1.310118
=> Done with 7 / 34  Batch Loss: 1.334079
=> Done with 8 / 34  Batch Loss: 1.376923
=> Done with 9 / 34  Batch Loss: 1.238375
=> Done with 10 / 34  Batch Loss: 1.394911
=> Done with 11 / 34  Batch Loss: 1.329300
=> Done with 12 / 34  Batch Loss: 1.368111
=> Done with 13 / 34  Batch Loss: 1.361318
=> Done with 14 / 34  Batch Loss: 1.361222
=> Done with 15 / 34  Batch Loss: 1.367690
=> Done with 16 / 34  Batch Loss: 1.351467
=> Done with 17 / 34  Batch Loss: 1.442819
=> Done with 18 / 34  Batch Loss: 1.241834
=> Done with 19 / 34  Batch Loss: 1.356354
=> Done with 20 / 34  Batch Loss: 1.365681
=> Done with 21 / 34  Batch Loss: 1.366064
=> Done with 22 / 34  Batch Loss: 1.412661
=> Done with 23 / 34  Batch Loss: 1.377641
=> Done with 24 / 34  Batch Loss: 1.264964
=> Done with 25 / 34  Batch Loss: 1.286786
=> Done with 26 / 34  Batch Loss: 1.329359
=> Done with 27 / 34  Batch Loss: 1.335322
=> Done with 28 / 34  Batch Loss: 1.294763
=> Done with 29 / 34  Batch Loss: 1.369597
=> Done with 30 / 34  Batch Loss: 1.254974
=> Done with 31 / 34  Batch Loss: 1.342323
=> Done with 32 / 34  Batch Loss: 1.253805
=> Done with 33 / 34  Batch Loss: 1.372427
=> Done with 34 / 34  Batch Loss: 1.381572
===> Avg. MSE Loss: 1.331228
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.578061
=> Done with 2 / 34  Batch Loss: 0.546506
=> Done with 3 / 34  Batch Loss: 0.541815
=> Done with 4 / 34  Batch Loss: 0.541456
=> Done with 5 / 34  Batch Loss: 0.517159
=> Done with 6 / 34  Batch Loss: 0.540193
=> Done with 7 / 34  Batch Loss: 0.606804
=> Done with 8 / 34  Batch Loss: 0.518404
=> Done with 9 / 34  Batch Loss: 0.630811
=> Done with 10 / 34  Batch Loss: 0.573134
=> Done with 11 / 34  Batch Loss: 0.537947
=> Done with 12 / 34  Batch Loss: 0.526276
=> Done with 13 / 34  Batch Loss: 0.554545
=> Done with 14 / 34  Batch Loss: 0.540155
=> Done with 15 / 34  Batch Loss: 0.536345
=> Done with 16 / 34  Batch Loss: 0.538102
=> Done with 17 / 34  Batch Loss: 0.538662
=> Done with 18 / 34  Batch Loss: 0.581039
=> Done with 19 / 34  Batch Loss: 0.563660
=> Done with 20 / 34  Batch Loss: 0.526593
=> Done with 21 / 34  Batch Loss: 0.548402
=> Done with 22 / 34  Batch Loss: 0.531775
=> Done with 23 / 34  Batch Loss: 0.528941
=> Done with 24 / 34  Batch Loss: 0.522328
=> Done with 25 / 34  Batch Loss: 0.543584
=> Done with 26 / 34  Batch Loss: 0.550907
=> Done with 27 / 34  Batch Loss: 0.558138
=> Done with 28 / 34  Batch Loss: 0.548331
=> Done with 29 / 34  Batch Loss: 0.538099
=> Done with 30 / 34  Batch Loss: 0.518825
=> Done with 31 / 34  Batch Loss: 0.510298
=> Done with 32 / 34  Batch Loss: 0.548625
=> Done with 33 / 34  Batch Loss: 0.516234
=> Done with 34 / 34  Batch Loss: 0.507230
===> Epoch 22 Complete: Avg. Loss: 0.544394
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.249233
=> Done with 2 / 34  Batch Loss: 1.241656
=> Done with 3 / 34  Batch Loss: 1.355604
=> Done with 4 / 34  Batch Loss: 1.204004
=> Done with 5 / 34  Batch Loss: 1.356872
=> Done with 6 / 34  Batch Loss: 1.293414
=> Done with 7 / 34  Batch Loss: 1.292638
=> Done with 8 / 34  Batch Loss: 1.287518
=> Done with 9 / 34  Batch Loss: 1.249892
=> Done with 10 / 34  Batch Loss: 1.199672
=> Done with 11 / 34  Batch Loss: 1.301117
=> Done with 12 / 34  Batch Loss: 1.269223
=> Done with 13 / 34  Batch Loss: 1.284611
=> Done with 14 / 34  Batch Loss: 1.436819
=> Done with 15 / 34  Batch Loss: 1.204665
=> Done with 16 / 34  Batch Loss: 1.402342
=> Done with 17 / 34  Batch Loss: 1.408668
=> Done with 18 / 34  Batch Loss: 1.259221
=> Done with 19 / 34  Batch Loss: 1.304169
=> Done with 20 / 34  Batch Loss: 1.268864
=> Done with 21 / 34  Batch Loss: 1.307943
=> Done with 22 / 34  Batch Loss: 1.301771
=> Done with 23 / 34  Batch Loss: 1.416700
=> Done with 24 / 34  Batch Loss: 1.389007
=> Done with 25 / 34  Batch Loss: 1.416272
=> Done with 26 / 34  Batch Loss: 1.381092
=> Done with 27 / 34  Batch Loss: 1.314143
=> Done with 28 / 34  Batch Loss: 1.126754
=> Done with 29 / 34  Batch Loss: 1.356889
=> Done with 30 / 34  Batch Loss: 1.343677
=> Done with 31 / 34  Batch Loss: 1.264645
=> Done with 32 / 34  Batch Loss: 1.374726
=> Done with 33 / 34  Batch Loss: 1.227295
=> Done with 34 / 34  Batch Loss: 1.177358
===> Avg. MSE Loss: 1.302014
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.567109
=> Done with 2 / 34  Batch Loss: 0.551793
=> Done with 3 / 34  Batch Loss: 0.531936
=> Done with 4 / 34  Batch Loss: 0.501621
=> Done with 5 / 34  Batch Loss: 0.542038
=> Done with 6 / 34  Batch Loss: 0.515248
=> Done with 7 / 34  Batch Loss: 0.518029
=> Done with 8 / 34  Batch Loss: 0.509109
=> Done with 9 / 34  Batch Loss: 0.537128
=> Done with 10 / 34  Batch Loss: 0.538260
=> Done with 11 / 34  Batch Loss: 0.510263
=> Done with 12 / 34  Batch Loss: 0.491600
=> Done with 13 / 34  Batch Loss: 0.526131
=> Done with 14 / 34  Batch Loss: 0.520953
=> Done with 15 / 34  Batch Loss: 0.553068
=> Done with 16 / 34  Batch Loss: 0.521274
=> Done with 17 / 34  Batch Loss: 0.513169
=> Done with 18 / 34  Batch Loss: 0.482966
=> Done with 19 / 34  Batch Loss: 0.525336
=> Done with 20 / 34  Batch Loss: 0.585317
=> Done with 21 / 34  Batch Loss: 0.496404
=> Done with 22 / 34  Batch Loss: 0.564493
=> Done with 23 / 34  Batch Loss: 0.533199
=> Done with 24 / 34  Batch Loss: 0.508111
=> Done with 25 / 34  Batch Loss: 0.501316
=> Done with 26 / 34  Batch Loss: 0.499601
=> Done with 27 / 34  Batch Loss: 0.502590
=> Done with 28 / 34  Batch Loss: 0.489650
=> Done with 29 / 34  Batch Loss: 0.523448
=> Done with 30 / 34  Batch Loss: 0.492899
=> Done with 31 / 34  Batch Loss: 0.527827
=> Done with 32 / 34  Batch Loss: 0.514127
=> Done with 33 / 34  Batch Loss: 0.511743
=> Done with 34 / 34  Batch Loss: 0.511815
===> Epoch 23 Complete: Avg. Loss: 0.521164
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.285908
=> Done with 2 / 34  Batch Loss: 1.377949
=> Done with 3 / 34  Batch Loss: 1.253281
=> Done with 4 / 34  Batch Loss: 1.342430
=> Done with 5 / 34  Batch Loss: 1.269212
=> Done with 6 / 34  Batch Loss: 1.349028
=> Done with 7 / 34  Batch Loss: 1.303606
=> Done with 8 / 34  Batch Loss: 1.279901
=> Done with 9 / 34  Batch Loss: 1.343931
=> Done with 10 / 34  Batch Loss: 1.443614
=> Done with 11 / 34  Batch Loss: 1.301464
=> Done with 12 / 34  Batch Loss: 1.294170
=> Done with 13 / 34  Batch Loss: 1.371211
=> Done with 14 / 34  Batch Loss: 1.261793
=> Done with 15 / 34  Batch Loss: 1.282810
=> Done with 16 / 34  Batch Loss: 1.148173
=> Done with 17 / 34  Batch Loss: 1.341420
=> Done with 18 / 34  Batch Loss: 1.304845
=> Done with 19 / 34  Batch Loss: 1.361398
=> Done with 20 / 34  Batch Loss: 1.261546
=> Done with 21 / 34  Batch Loss: 1.215900
=> Done with 22 / 34  Batch Loss: 1.272016
=> Done with 23 / 34  Batch Loss: 1.367013
=> Done with 24 / 34  Batch Loss: 1.459921
=> Done with 25 / 34  Batch Loss: 1.266739
=> Done with 26 / 34  Batch Loss: 1.281301
=> Done with 27 / 34  Batch Loss: 1.236807
=> Done with 28 / 34  Batch Loss: 1.277578
=> Done with 29 / 34  Batch Loss: 1.261172
=> Done with 30 / 34  Batch Loss: 1.297320
=> Done with 31 / 34  Batch Loss: 1.250989
=> Done with 32 / 34  Batch Loss: 1.232031
=> Done with 33 / 34  Batch Loss: 1.292036
=> Done with 34 / 34  Batch Loss: 1.284435
===> Avg. MSE Loss: 1.299204
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.498607
=> Done with 2 / 34  Batch Loss: 0.486832
=> Done with 3 / 34  Batch Loss: 0.521779
=> Done with 4 / 34  Batch Loss: 0.498510
=> Done with 5 / 34  Batch Loss: 0.552290
=> Done with 6 / 34  Batch Loss: 0.567255
=> Done with 7 / 34  Batch Loss: 0.517588
=> Done with 8 / 34  Batch Loss: 0.534210
=> Done with 9 / 34  Batch Loss: 0.508323
=> Done with 10 / 34  Batch Loss: 0.532820
=> Done with 11 / 34  Batch Loss: 0.517230
=> Done with 12 / 34  Batch Loss: 0.485021
=> Done with 13 / 34  Batch Loss: 0.531938
=> Done with 14 / 34  Batch Loss: 0.491327
=> Done with 15 / 34  Batch Loss: 0.515707
=> Done with 16 / 34  Batch Loss: 0.535866
=> Done with 17 / 34  Batch Loss: 0.495884
=> Done with 18 / 34  Batch Loss: 0.520095
=> Done with 19 / 34  Batch Loss: 0.601229
=> Done with 20 / 34  Batch Loss: 0.499819
=> Done with 21 / 34  Batch Loss: 0.500917
=> Done with 22 / 34  Batch Loss: 0.494137
=> Done with 23 / 34  Batch Loss: 0.479025
=> Done with 24 / 34  Batch Loss: 0.491528
=> Done with 25 / 34  Batch Loss: 0.526909
=> Done with 26 / 34  Batch Loss: 0.505213
=> Done with 27 / 34  Batch Loss: 0.478135
=> Done with 28 / 34  Batch Loss: 0.490517
=> Done with 29 / 34  Batch Loss: 0.515847
=> Done with 30 / 34  Batch Loss: 0.537557
=> Done with 31 / 34  Batch Loss: 0.484709
=> Done with 32 / 34  Batch Loss: 0.490164
=> Done with 33 / 34  Batch Loss: 0.456119
=> Done with 34 / 34  Batch Loss: 0.472729
===> Epoch 24 Complete: Avg. Loss: 0.509878
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.380140
=> Done with 2 / 34  Batch Loss: 1.329431
=> Done with 3 / 34  Batch Loss: 1.260158
=> Done with 4 / 34  Batch Loss: 1.456355
=> Done with 5 / 34  Batch Loss: 1.356876
=> Done with 6 / 34  Batch Loss: 1.537987
=> Done with 7 / 34  Batch Loss: 1.253078
=> Done with 8 / 34  Batch Loss: 1.288199
=> Done with 9 / 34  Batch Loss: 1.257296
=> Done with 10 / 34  Batch Loss: 1.292049
=> Done with 11 / 34  Batch Loss: 1.334389
=> Done with 12 / 34  Batch Loss: 1.295282
=> Done with 13 / 34  Batch Loss: 1.344164
=> Done with 14 / 34  Batch Loss: 1.265323
=> Done with 15 / 34  Batch Loss: 1.393110
=> Done with 16 / 34  Batch Loss: 1.328913
=> Done with 17 / 34  Batch Loss: 1.376094
=> Done with 18 / 34  Batch Loss: 1.134102
=> Done with 19 / 34  Batch Loss: 1.487044
=> Done with 20 / 34  Batch Loss: 1.416430
=> Done with 21 / 34  Batch Loss: 1.305410
=> Done with 22 / 34  Batch Loss: 1.250267
=> Done with 23 / 34  Batch Loss: 1.249368
=> Done with 24 / 34  Batch Loss: 1.422878
=> Done with 25 / 34  Batch Loss: 1.272465
=> Done with 26 / 34  Batch Loss: 1.412320
=> Done with 27 / 34  Batch Loss: 1.452511
=> Done with 28 / 34  Batch Loss: 1.303189
=> Done with 29 / 34  Batch Loss: 1.218180
=> Done with 30 / 34  Batch Loss: 1.206784
=> Done with 31 / 34  Batch Loss: 1.183623
=> Done with 32 / 34  Batch Loss: 1.335499
=> Done with 33 / 34  Batch Loss: 1.331437
=> Done with 34 / 34  Batch Loss: 1.183199
===> Avg. MSE Loss: 1.320987
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.499011
=> Done with 2 / 34  Batch Loss: 0.544694
=> Done with 3 / 34  Batch Loss: 0.539979
=> Done with 4 / 34  Batch Loss: 0.480201
=> Done with 5 / 34  Batch Loss: 0.509176
=> Done with 6 / 34  Batch Loss: 0.469303
=> Done with 7 / 34  Batch Loss: 0.500225
=> Done with 8 / 34  Batch Loss: 0.501341
=> Done with 9 / 34  Batch Loss: 0.486142
=> Done with 10 / 34  Batch Loss: 0.477555
=> Done with 11 / 34  Batch Loss: 0.478705
=> Done with 12 / 34  Batch Loss: 0.514802
=> Done with 13 / 34  Batch Loss: 0.453156
=> Done with 14 / 34  Batch Loss: 0.501940
=> Done with 15 / 34  Batch Loss: 0.462204
=> Done with 16 / 34  Batch Loss: 0.471680
=> Done with 17 / 34  Batch Loss: 0.472992
=> Done with 18 / 34  Batch Loss: 0.495388
=> Done with 19 / 34  Batch Loss: 0.476202
=> Done with 20 / 34  Batch Loss: 0.478386
=> Done with 21 / 34  Batch Loss: 0.482293
=> Done with 22 / 34  Batch Loss: 0.474254
=> Done with 23 / 34  Batch Loss: 0.486667
=> Done with 24 / 34  Batch Loss: 0.506420
=> Done with 25 / 34  Batch Loss: 0.505183
=> Done with 26 / 34  Batch Loss: 0.520495
=> Done with 27 / 34  Batch Loss: 0.480389
=> Done with 28 / 34  Batch Loss: 0.538165
=> Done with 29 / 34  Batch Loss: 0.496912
=> Done with 30 / 34  Batch Loss: 0.569744
=> Done with 31 / 34  Batch Loss: 0.501268
=> Done with 32 / 34  Batch Loss: 0.460416
=> Done with 33 / 34  Batch Loss: 0.489116
=> Done with 34 / 34  Batch Loss: 0.484264
===> Epoch 25 Complete: Avg. Loss: 0.494373
===> Evaluating Model
=> Done with 1 / 34  Batch Loss: 1.235203
=> Done with 2 / 34  Batch Loss: 1.340928
=> Done with 3 / 34  Batch Loss: 1.440281
=> Done with 4 / 34  Batch Loss: 1.322508
=> Done with 5 / 34  Batch Loss: 1.364007
=> Done with 6 / 34  Batch Loss: 1.286874
=> Done with 7 / 34  Batch Loss: 1.305748
=> Done with 8 / 34  Batch Loss: 1.300520
=> Done with 9 / 34  Batch Loss: 1.341783
=> Done with 10 / 34  Batch Loss: 1.322919
=> Done with 11 / 34  Batch Loss: 1.291793
=> Done with 12 / 34  Batch Loss: 1.387230
=> Done with 13 / 34  Batch Loss: 1.360024
=> Done with 14 / 34  Batch Loss: 1.309256
=> Done with 15 / 34  Batch Loss: 1.414731
=> Done with 16 / 34  Batch Loss: 1.317823
=> Done with 17 / 34  Batch Loss: 1.222693
=> Done with 18 / 34  Batch Loss: 1.396131
=> Done with 19 / 34  Batch Loss: 1.326254
=> Done with 20 / 34  Batch Loss: 1.373190
=> Done with 21 / 34  Batch Loss: 1.369367
=> Done with 22 / 34  Batch Loss: 1.424310
=> Done with 23 / 34  Batch Loss: 1.260290
=> Done with 24 / 34  Batch Loss: 1.238867
=> Done with 25 / 34  Batch Loss: 1.329647
=> Done with 26 / 34  Batch Loss: 1.425482
=> Done with 27 / 34  Batch Loss: 1.318872
=> Done with 28 / 34  Batch Loss: 1.325518
=> Done with 29 / 34  Batch Loss: 1.404566
=> Done with 30 / 34  Batch Loss: 1.253563
=> Done with 31 / 34  Batch Loss: 1.405288
=> Done with 32 / 34  Batch Loss: 1.481977
=> Done with 33 / 34  Batch Loss: 1.377660
=> Done with 34 / 34  Batch Loss: 1.204854
===> Avg. MSE Loss: 1.337652
===> Learning Rate = 0.0001
=> Done with 1 / 34  Batch Loss: 0.465204
=> Done with 2 / 34  Batch Loss: 0.489669
=> Done with 3 / 34  Batch Loss: 0.461032
=> Done with 4 / 34  Batch Loss: 0.545781
=> Done with 5 / 34  Batch Loss: 0.456117
=> Done with 6 / 34  Batch Loss: 0.510097
=> Done with 7 / 34  Batch Loss: 0.489327
=> Done with 8 / 34  Batch Loss: 0.505026
=> Done with 9 / 34  Batch Loss: 0.467910
=> Done with 10 / 34  Batch Loss: 0.499537
=> Done with 11 / 34  Batch Loss: 0.503268
=> Done with 12 / 34  Batch Loss: 0.497355
=> Done with 13 / 34  Batch Loss: 0.508939
=> Done with 14 / 34  Batch Loss: 0.469459
=> Done with 15 / 34  Batch Loss: 0.498067
=> Done with 16 / 34  Batch Loss: 0.473450
slurmstepd: error: *** JOB 33305 ON rigveda CANCELLED AT 2020-06-21T16:16:35 ***
